\documentclass[twocolumn]{article}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{color}
\usepackage{authblk}
\usepackage[colorlinks,citecolor=red,urlcolor=blue,bookmarks=false,hypertexnames=true]{hyperref}
\usepackage{geometry}
\usepackage{float}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}

\newcommand{\limite}{\displaystyle\lim}
\newcommand{\integral}{\displaystyle\int}
\newcommand{\somatorio}{\displaystyle\sum}
\newtheorem{teorema}{Teorema}[section]
\newtheorem{definicao}{Definição}[section]
\newtheorem{proposicao}{Proposição}[section]
\newtheorem{corolario}[teorema]{Corolário}
\newtheorem{lema}{Lema}[section]
\newtheorem{exemplo}{Exemplo}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{collorary}[theorem]{Collorary}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Propositon}[section]
\newtheorem{example}{Example}[section]

% Dados de identificação
\title{Notas sobre Redes Neurais}
\author{Patrick Oliviera}
\affil{.}

\begin{document}
\maketitle

\section{Introdução}
 Anotações que faço conforme estudo Redes Neurais.

 \subsection{Benefits of Neural Networks}
	 A neural network devires its computing power through, first, its massively parallel distributed structure and, second, its ability to learn and therefore generalize. Generalization refers to the neural network's production of reasonable outputs for inputs not encountered during training (learning).

	 \begin{enumerate}
		 \item Nonlinearity - An artificial neuron can be linear or nonlinear. A neural network made up of an interocnnection of nonlinear neurons, is itself nonlinear. (?) What characterizes a nonlinear neuron?
		       Nonlinearity is a highly important property, particularly if the underlying physical mechanism responsible for the generation of the input signal is inherently nonlinear. (?) There are studies about applications of neural networks to series originated from chaotic systems?
		 \item  Input-Output Mapping - Supervised learning, involves modification of the synaptic weights of a neural network by applying a set of labeled training examples, or task examples. Each example consists of a unique input signal and a corresponding desired (target) response. The nework is presented with an example picked at random from the set, and the synaptic weights (free parameters) of the network are modified to minimize the difference between the desired response and the actual response of the network produced by the input signal in accordance with an appropriate statistical criterion. The training of the network is repeated for many examples in the set, until the network reaches a steady state where there are no further significant changes in the synaptic weights. The previously applied training examples may be reapplied during the training sesion, but in a different order. Thus the network learns from the examples by constructing an input-ouput mapping for the problem at hand.
		       The network learns to represent associations between elements (inputs and outputs) with functions (mappings) that are written in vector or matricial form.
		 \item Adaptativy - Neural networks have a built-in capability to adapt their synaptic weights to changes in the surrounding environment. In particular, a neural network trianed to operate in a specific environment can be easily retrained to deal with minor changes in the operating environmental conditions. Moreover, when it is operaitng in a nonstationary environment (i.e., one where statistics change with time), a neural network may be designed to change its synaptic weights in real time.
		 \item Evidential Response - In the context of pattern classification, a neural network can be designed to provide information not only about which particular pattern to select but also about the confidence in the deicision made.
		 \item Contextual Information -(?) Knowledge is represented by the very structure and activation state of a neural network. Every neuron in the network is potentially affected by the global activity of all other neurons in the network. Consequently, contextual information is dealt with naturally by a neural network.
		 \item Fault Tolerance - A neural network, implemented in hardware form, has the potential to be inherently fault tolerant, or capable of robust computation, in the sense that its performance degrades gracefully under adverse operating conditions.
		 \item VLSI Implementability -(?) The massively parallel nature of a neural network makes it potentially fast for the computation of certain tasks. This same feature makes a neural network well suited for implementation using very-large-scale-integrated (VLSI) technology.
		 \item Uniformity of Analysis and Design - Basically, neural networks enjoy universality as information processors.
		       \begin{enumerate}
			       \item Neurons, in one form or another, represent an ingredient common to all neural networks.
			       \item This commonality makes it possible to share theories and learning algorithms in different applications of neural networks.
			       \item Modular networks can be built through a seamless integration of modules.
		       \end{enumerate}
	 \end{enumerate}

\section{Model of a Neuron}

 \begin{figure}[H]
	 \center
	 \includegraphics[scale = 0.4]{\Figures\modelOfaNeuron.png}
	 \caption{Mathematical model of a neuron.}
	 \label{modelofaneuron}
 \end{figure}

 A neuron is an information-processing unit fundamental to the operation of a neural network. It is composed by

 \begin{enumerate}
	 \item A set of \textit{synapses}, or \textit{connecting links}, each of which is characterized by a \textit{weight} or \textit{strength} of its own. Specifically, a signal $ x_j $ at the input of synapse $ j $ connected to neuron $ k $ is multiplied by the synaptic weight $ w_{kj} $.
	 \item An \textit{adder} for summing the input signals, weighted by the respective synaptic strengths of the neuron. This operations constitutes a \textit{linear combiner}.
	 \item An \textit{activation function} for limiting the amplitude of the output of a neuron. It is also referred to as a \textit{squashing function}.
 \end{enumerate}

 It can be represented mathematically by

 $$ \begin{aligned}[t]
		 v_{k} & = u_{k} + b_{k}                             \\
		       & = \somatorio_{j = 1}^{m}w_{kj}x_{j} + b_{k} \\
		       & = \somatorio_{j = 0}^{m}w_{kj}x_{j}         \\
		       & = \vec{w}^{T}\cdot \vec{x}                  \\
	 \end{aligned} $$

 \noindent where

 $$ \vec{w} =
	 \begin{bmatrix}
		 1      \\
		 \vdots \\
		 w_{km} \\
	 \end{bmatrix}
	 , \;\;\;
	 \vec{x} =
	 \begin{bmatrix}
		 b_{k}  \\
		 \vdots \\
		 x_{m}  \\
	 \end{bmatrix}
 $$

 \noindent and the output is written as

 $$ y_{k} = \varphi (v_{k}) $$

 The term $ v_k $ is called \textit{induced local field}, or \textit{activation potential}.

 \subsection{Types of Activation Function}

	 \begin{enumerate}
		 \item Threshold Function - A neuron that implements this type of activation function is called \textit{McCulloch - Pitts neuron}. The function is given by

		       $$ \varphi (v) =
			       \begin{cases}
				       1 \;\;\; if v \geq 0 \\
				       0 \;\;\; if v < 0    \\
			       \end{cases}
		       $$,

		       \noindent as a result, the output will be expressed by

		       $$ y_k =
			       \begin{cases}
				       1 \;\;\; if v_{k} \geq 0 \\
				       0 \;\;\; if v_{k} < 0
			       \end{cases}
		       $$

		 \item Sigmoid Function - It uses the \textit{logistic function} as the activation function, defined by

		       $$ \varphi (v) = \dfrac{1}{1 + \exp (-av)} $$

		       We can note that if $ a \rightarrow \infty $, we obtain the previous activation function. By allowing negative values for the activation function, its result will be therefore defined by

		       $$ \varphi (v) =
			       \begin{cases}
				       1 \;\;\; \text{if} v > 0  \\
				       0 \;\;\; \text{if} v = 0  \\
				       -1 \;\;\; \text{if} v < 0 \\
			       \end{cases}
		       $$

		       \noindent which is commonly referred to as the signum function.
	 \end{enumerate}

 \subsection{Stochastic Model of a Neuron}
	 The neural model previously described is deterministic in that its input-output behavior is precisely defined for all inputs. For some applications (?) it is desirable to base the analysis on a stochastic neural model. We can modify the McCulloch-Pitts model by making the decision for a neuron to fire probabilistic. By denoting $ P (v) $ as the probability of firing, we define

	 $$x =
		 \begin{cases}
			 +1 \;\;\; \text{with probability } P (v)     \\
			 -1 \;\;\; \text{with probability } 1 - P (v) \\
		 \end{cases}
	 $$

	 A standard choice for $ P (v) $ is the sigmoid-shaped function

	 $$ P (v) = \dfrac{1}{1 + \exp (-v / T)} $$

	 \noindent where $ T $ is a \textit{pseudotemperature} used to control the noise level and therefore the uncertainty in firing.

 \subsection{Neural Networks Viewed as Directed Graphs}

	 A \textit{signal-flow graph} is a network of directed \textit{links (branches)} that are interconnected at certain points called \textit{nodes}. A typical node $ j $ has an associated \textit{node signal} $ x_j $. A typical directed link originates at node $ j $ and terminates on node $ k $; it has an associated \textit{transfer function}, or \textit{transmittance}, that specifies the manner in which the signal $ y_k $ at node $ k $ depends on the signal $ x_j $ at node $ j $. The flow of signals in the various parts of the graph is dictated by three basic rules:

	 \begin{enumerate}
		 \item A signal flows along a link only in the direction defined by the arrow on the link.
		 \item A node signal equals the algebraic sum of all signals entering the pertinent node via the incoming links.
		 \item The signal at a node is transmitted to each outgoing link originating from that node, with the transmission being entirely independent of the transfer functions of the outgoing links.
	 \end{enumerate}

	 \begin{figure}
		 \center
		 \includegraphics[scale = 0.5]{\Figures\flowgraphrules.png}
		 \caption{Picture depiction of the three rules for signal-flow graphs.}
		 \label{flowgraphrules}
	 \end{figure}

	 Based on those rules, we make the following mathematical definiton of a neural network:

	 A neural network is a directed graph consisting of nodes with interconnecting synaptic and activation links and is characterized by four properties:

	 \begin{enumerate}
		 \item Each neuron is represented by a set of linear synaptic links, an externally applied bias, and a possibly nonlinear activation link. The bias is represented by a synaptic link connected to an inpu fixed at +1.
		 \item The synaptic links of a neuron weight their respective input signals.
		 \item The weighted sum of the input signals defines the induced local field of the neuron in question.
		 \item The activation link squashes the induced local field of the neuron to produce an output.
	 \end{enumerate}

	 \begin{figure}[H]
		 \center
		 \includegraphics[scale = 0.35]{\Figures\networkNeuronModel.png}
		 \caption{Signal-flow network model for a neuron.}
		 \label{networkNeuronModel}
	 \end{figure}

	 We can also define a simplified network model using a \textit{partially complete graph}, characterized by

	 \begin{enumerate}
		 \item \textit{Source nodes} supply input signals to the graph.
		 \item Each neuron is represented by a single node called a \textit{computation node}.
		 \item The \textit{communication links} interconnecting the source and computation nodes of the graph carry no weight; they merely provide directions of signal flow in the graph.
	 \end{enumerate}

	 A partially complete directed graph defined in this way is referred to as an \textit{architectural graph}, describing the layout of the neural network.

	 \begin{figure}[H]
		 \center
		 \includegraphics[scale = 0.4]{\Figures\architecturalgraph.png}
		 \caption{Archictectural graph of a neuron.}
		 \label{architecturalgraph}
	 \end{figure}

 \subsection{Network Architectures}
	 The manner in which the neurons of a neural network are structured is intimately linked with the learning algorithm used to train the network. In general, we may identify three fundamentally different classes of network architectures:

	 \subsubsection{Single-Layer Feedforward Networks}
		 In a \textit{layered} neural network, the neurons are organized in the form of layers. In the simplest form of a layered network, we have an \textit{input layer} of source nodes that projects directly onto an \textit{output layer} of neurons (computation nodes), but not vice versa. In other words, this network is strictly of a \textit{feedforward type}. Such network is called a \textit{single-layer network}, with the designation "single-layer" referring to the output layer of computation nodes (neurons). We do not count the input layer of source nodes because no computation is performed there

		 \begin{figure}
			 \center
			 \includegraphics[scale = 0.7]{\Figures\singleLayerNetwork.png}
			 \caption{Feedforward network with a single layer of neurons.}
			 \label{singleLayerNetwork}
		 \end{figure}

	 \subsubsection{Multilayer Feedforward Networks}
		 The second class of a feedforward neural network distinguishes itself by the presence of one or more \textit{hidden layers}, whose computation nodes are correspondingly called \textit{hidden neurons} or \textit{hidden units}. The function of hidden neurons is to intervene between the external input and the network output in some useful manner. By adding one or more hidden layers, the network is enabled to extract higher-order statistics from its input. Typically, the neurons in each layer of the network have as their inputs the output signals of the preceding layer only.

 \subsection{Knowledge Representation}
	 \textit{Knowledge refers to stored information or models used by a person or machine to interpret, predict, and appropriately respond to the outside world.}

\section{Rosenblatt's Perceptron}
 Rosenblatt's perceptron is built around a nonlinear neuron, namely, the \textit{McCulloch-Pitts model} of a neuron. The goal of the perceptron is to correctly classify the set of externally applied stimuli $ x_1, x_2, \ldots, x_m $ into one of two classes $ \mathcal{C}_{1} $ or $ \mathcal{C}_{2} $, if the perceptron output is, respectively, $ +1 $ or $ -1 $. In the simplesment form of the perceptron, there are two decision regions separated by a \textit{hyperplane}, which is defined by

 $$ \somatorio_{i = 1}^{m}w_{i}x_{i} = 0 $$

 \begin{figure}[H]
	 \center
	 \includegraphics[scale = 0.4]{\Figures\linearDecisionRegion.png}
	 \caption{Illustration of the hyperplane (in 2 dimensions) as decision boundary.}
	 \label{linearDecisionRegion}
 \end{figure}

 The synaptic weights $ w_1, w_2, \ldots, w_m $ of the perceptron can be adapted on an iteration-by-iteration basis.

 \subsection{The Perceptron Convergence Theorem}
	 We'll be using vector notation, i.e.,

	 $$
		 \vec{x}(n) =
		 \begin{bmatrix}
			 +1        \\
			 x_{1} (n) \\
			 x_{2} (n) \\
			 \vdots    \\
			 x_{m} (n) \\
		 \end{bmatrix}
		 , \;\;\;
		 \vec{w} (n) =
		 \begin{bmatrix}
			 b         \\
			 w_{1} (n) \\
			 w_{2} (n) \\
			 \vdots    \\
			 w_{m} (n) \\
		 \end{bmatrix}
	 $$

	 $$
		 v(n) = \vec{w}^{T}(n) \vec{x} (n)
	 $$

	 For fixed $ n $, the equation $ \vec{w}^{T} \vec{x} = 0 $, plotted in an m-dimensional space with coordinates $ x_{1}, x_{2}, \ldots, x_{m} $, defines a hyperplane as the decision surface between two different classes of inputs. For the perceptron to function properly, the two classes $ \mathcal{C}_{1} $ e $ \mathcal{C}_{2} $ must be \textit{linearly separable}. If $ \mathcal{H}_{1} $ and $ \mathcal{H}_{2} $ are the set of inputs that belong to the classes $ \mathcal{C}_{1} $ and $ \mathcal{C}_{2} $, respectively, then the training process involves the adjustment of the weight vector $ \vec{w} $ in such a way that the two classes are linearly separable. That is, there exists a weight vector $ \vec{w} $ such that we may state

	 $$ \begin{aligned}[t]
			  & \vec{w}^{T} \vec{x} > 0 \text{ for every input vector $ \vec{x} $ belonging to $ \mathcal{C}_{1} $}         \\
			  & \vec{w}^{T} \vec{x} \leq 0 \text{ for every input vector } \vec{x} \text{ belonging to $ \mathcal{C}_{2} $} \\
		 \end{aligned} $$


 \subsection{Relation Between the Perceptron and Bayes Classifier for a Gaussian Environment}

	 \subsubsection{Bayes Classifier}

		 In the \textit{Bayes Classifier}, or \textit{Bayes hyphothesis testing procedure}, we minimize the \textit{average risk}, denoted by \( \mathcal{R} \). For a two class problem, the average risk is defined by

		 $$ \begin{aligned}[t]
				 \mathcal{R} & = c_{11}p_{1}\integral_{ \mathcal{H}_{1}}p_{X} (x | \mathcal{C}_{1})dx + c_{22}p_{2}\integral_{ \mathcal{H}_{2}}p_{X} (x | \mathcal{C}_{2})dx  \\
				             & + c_{21}p_{1}\integral_{ \mathcal{H}_{2}}p_{X} (x | \mathcal{C}_{1})dx + c_{12}p_{2}\integral_{ \mathcal{H}_{1}} p_{X} (x | \mathcal{C}_{2})dx \\
			 \end{aligned} $$

		 \noindent where the various terms are defined as follows

		 \begin{itemize}
			 \item $ p_{i} =$ \textit{prior probability} that the observation vector \textbf{x} corresponds to an object in class $ \mathcal{C}_{1} $.
			 \item $ c_{ij} =$ cost of deciding in favor of class $ \mathcal{C}_{i} $ represented by subspace $ \mathcal{H}_{i} $ when class $ \mathcal{C}_{j} $ is true.
			 \item $ p_{X} (x | \mathcal{C}_{i}) =$ conditional probability density function of the random vector X, given that the observation vector x corresponds to an object in class $ \mathcal{C}_{1} $.
			 \item $ c_{11}p_{1}\integral_{ \mathcal{H}_{1}} p_{X} (x | \mathcal{C}_{1})dx =$ Total risk of deciding in favor of class $ \mathcal{C}_{1} $ when it's actually the case.
			 \item $ c_{12}p_{2}\integral_{ \mathcal{H}_{2}} p_{X} (x | \mathcal{C}_{2})dx =$ Total risk of deciding in favor of class $ \mathcal{C}_{2} $ when the correct one is $ \mathcal{C}_{1} $.
		 \end{itemize}

		 The minimization of the average risk is done by finding the correct decision costs. I understand that i should minimize the risk of making the incorrect decision, but why we understand the total proportion of correct decisions as a risk? And why it should be minimized? The integrals are fixed values, depending only on the data set, or they are choosen also?
		 We can rewrite the previous equation in the equivalent form

		 $$ \begin{aligned}[t]
				 \mathcal{R} & = c_{11}p_{1}\integral_{ \mathcal{H}_{1}}p_{X} (x | \mathcal{C}_{1})dx + c_{22}p_{2}\integral_{ \mathcal{H} - \mathcal{H}_{1}}p_{X} (x | \mathcal{C}_{2})dx  \\
				             & + c_{21}p_{1}\integral_{ \mathcal{H}_{2}}p_{X} (x | \mathcal{C}_{1})dx + c_{12}p_{2}\integral_{ \mathcal{H} - \mathcal{H}_{1}} p_{X} (x | \mathcal{C}_{2})dx \\
			 \end{aligned} $$

		 \noindent where $ \mathcal{H} = \mathcal{H}_{1} \mathcal{H}_{2} $, and $ c_{11} < c_{21} $ and $ c_{22} < c_{12} $ (why?). This equation reduces to

		 $$ \begin{aligned}[t]
				  & \mathcal{R} = c_{21}p_{1} + c_{22}p_{2}                                                                                                      \\
				  & + \integral_{ \mathcal{H}_{1} }[ p_{2} (c_{12} - c_{22})p_{X} (x | \mathcal{C}_{2}) - p_{1} (c_{21} - c_{11})p_{X} (x | \mathcal{C}_{1}) ]dx \\
			 \end{aligned}$$



		 The first two terms on the right-hand side represent a fixed cost related to the decision in favor of class $ \mathcal{C}_{2} $. The optimization of the average risk, therefore, lies in deciding the classification of values that will make the integral at the right-hand side be a negative value. With this idea we can formulate the Bayes classifier as follows

		 \begin{definicao}
			 If the condition
			 $$ p_{1} (c_{21} - c_{11}) p_{X} (x | \mathcal{C}_{1}) > p_{2} (c_{12} - c_{22}) p_{X} (x | \mathcal{C}_{2}) $$
			 \noindent holds, assign the obsrevation vector $ x $ to subspace $ \mathcal{H}_{1} $ (i.e., class $ \mathcal{C}_{1} $). Otherwise assign $ x $ to $ \mathcal{H}_{2} $ (i.e., class $ \mathcal{C}_{2} $).
		 \end{definicao}

		 To simplify matters, define


		 $$ \Lambda (x) = \dfrac{p_{X} (x | \mathcal{c}_{1})}{ p_{X} (x | \mathcal{C}_{2})} $$
		 \noindent and

		 $$ \xi = \dfrac{p_{2} (c_{12} - c_{22})}{p_{1} (c_{21} - c_{11})} $$

		 The quantity $ \Lambda (x) $ is called likelihood ratio. The quantity $ \xi $ is called the \textit{threshold} of the test. This gives us an equivalent definition

		 \begin{definicao}
			 If, for an observation vector $ x $, we have $ \Lambda (x) > \xi $, assign $ x $ to class $ \mathcal{C}_{1} $. Otherwise, assign it to class $ \mathcal{C}_{2} $.
		 \end{definicao}

	 \subsubsection{Bayes Classifier for a Gaussian Distribution}
		 Consider now the special case of a two-class problem, for which the underlying distribution is Gaussian. The problem consists in diciding whether or not an observation belongs to the distribution (class) $ \mathcal{C}_{1} $ or $ \mathcal{C}_{2} $. We assume that they differ in their mean but they have equal covariance matrices.

		 $$ \begin{aligned}[t]
				 \text{Class } \mathcal{C}_{1}: & \mathbb{E} [X] = \mu_{1}                          \\
				                                & \mathbb{E} [ (X - \mu_{1}) (X - \mu_{1})^{T}] = C \\
			 \end{aligned} $$
		 $$ \begin{aligned}[t]
				 \text{Class } \mathcal{C}_{2}: & \mathbb{E} [X] = \mu_{2}                          \\
				                                & \mathbb{E} [ (X - \mu_{2}) (X - \mu_{2})^{T}] = C \\
			 \end{aligned} $$

		 The covariance matrix $ C $ is nondiagonal and nonsingular. The probability density function of $ X $ as the multivariate Gaussian distribution will be given by

		 $$ p_{X} (x | \mathcal{C}_{i}) = \dfrac{ e^{\left( -\dfrac{1}{2} (x - \mu_{i})^{T}C^{-1} (x - \mu_{i})\right)}}{ (2\pi)^{m/2} (\det C)^{1/2}} $$

		 We further assume that

		 \begin{itemize}
			 \item The two classes are equiprobable, i.e.,
			       $$ p_{1} = p_{2} = \dfrac{1}{2} $$
			 \item Misclassifications carry the same cost, and no cost is incurred on correct classifications.
		 \end{itemize}

		 Using the definition of the Bayes Classifier (log-based), we obtain that

		 $$ \log \Lambda (x) = (\mu_{1} - \mu_{2})^{T} C^{-1} x + \dfrac{1}{2} (\mu_{2}^{T} C^{-1} \mu_{2} - \mu^{T}_{1} C^{-1}\mu_{1}) $$

		 \noindent and

		 $$ \log \xi = 0 $$

		 Therefore, the Bayes Classifier for a Gaussian distribution is a \textit{linear classifier}, as described by the relation

		 $$ y = w^{T}x + b $$

		 \noindent where

		 $$ \begin{aligned}[t]
				  & y = \log \Lambda (x)                                                      \\
				  & w = C^{-1} (\mu_{1} - \mu_{2})                                            \\
				  & b = \dfrac{1}{2} (\mu_{2}^{T} C^{-1} \mu_{2} - \mu_{1}^{T} C^{-1}\mu_{1}) \\
			 \end{aligned} $$

		 The operation of the Bayes classifier for the Gaussian environment described herein is analogous to that of the perceptron in that they are both linear classifiers. There are, however, some subtle and important differences between them:

		 \begin{itemize}
			 \item The perceptron operates on the premise that the patterns to be classified are \textit{linearly separable}. The Gaussian distributions of the two patterns assumed in the derivation of the Bayes classifier certainly do \textit{overlap} each other and are therefore \textit{not} separable. The extent of the overlap is determined by the mean vectors and the covariance matrix. When the inputs are nonseparable and their distributions overlap, the perceptron convergence algorithm develops a problem because decision boundaries between the different classes may oscilate continuously.
			 \item The Bayes classifier minimizes the probability of classification error, and this minimization is independent of the overlap between the underlying Gaussian distributions of the two classes.
			 \item The perceptron convergence algorithm is \textit{nonparametric} in the sense that it makes no assumptions concerning the form of the underlying distributions. It operates by concentrating on errors that occur where the distributions overlap. In contrast, the Bayes classifier is \textit{parametric}; its derivation is contingent on the assumption that the underlying distributions be Gaussian.
			 \item The perceptron convergence algorithm is both adaptive and simple to implement; its storage requirement is confined to the set of synaptic weights and bias. On the other hand, the design of the Bayes classifier is fixed; it can be made adaptive, but at the expense of increase storage requirements and more complex computations.
		 \end{itemize}

 \subsection{The Batch Perceptron Algorithm}

	 (?) What does "Batch" means?

	 The cost function we have in mind is a function that permits the application of a gradient search. Specifically, we define the \textit{perceptron cost function} as

	 $$ J(w) = \sum_{x(n) \in \mathcal{H}} ( -w^{T} x(n) d(n)) $$

	 \noindent where $ \mathcal{H} $ is the set of samples $ x $ \textit{misclassified} by a perceptron using $ w $ as its weight vector. This cost function is \textit{differentiable} with respect to the weight vector $ w $. Thus, differentiating $ J(w) $ in respect to $ w $ yields the \textit{gradient vector}

	 $$ \nabla J(w) = \sum_{x(n) \in \mathcal{H}} (-x(n)d(n)) $$

	 In the \textit{method of steepest descent}, the adjustment to the weight vector $ w $ at each time-step of the algorithm is applied in a direction \textit{opposite} to the gradient vector $ \nabla J(w) $. The algorithm takes the form

	 $$ \begin{aligned}[t]
			 w(n+1) & = w(n) - \eta (n) \nabla J(w)                           \\
			        & = w(n) + \eta (n) \sum_{x(n) \in \mathcal{H}} x(n) d(n) \\
		 \end{aligned} $$

	 \noindent which includes the single-sample correction version of the perceptron convergence algorithm as a special case. The algorithm is said to be of the "batch" kind because at each time-step of the algorithm, a batch of missclassified samples is used to compute the adjustment.

\section{Multilayer Perceptron}

 The following three points highlight the basic features of multilayer perceptrons:

 \begin{itemize}
	 \item The model of each neuron in the network includes a nonlinear activation function that is \textit{differentiable}.
	 \item The network contains one or more layers that are \textit{hidden} from both the input and output nodes.
	 \item The network exhibits a high degree of \textit{connectivity}, the extent of which is determined by synaptic weights of the network.
 \end{itemize}

 A popular method for the trianing of multilayer perceptrons is the back-propagation algorithm. The trianing proceeds in two phases:
 \begin{enumerate}
	 \item In the \textit{forward phase}, the synaptic weights of the network are fixed and the input signal is propagated through the network, layer by layer, until it reaches the output. Thus, in this phase, changes are confined to the activation potentials and outputs of the neurons in the network.
	 \item In the \textit{backward phase}, an error signal is produced by comparing the output of the network with a desired response. The resulting error signal is propagated through the network, agian layer by layer, but this time the propagation is performed in the backward direction. In this second phase, successive adjustments are made to the synaptic weights of the network.
 \end{enumerate}

 \begin{figure}[H]
	 \center
	 \includegraphics[scale = 0.4]{\Figures\multilayerPerceptron.png}
	 \caption{Architectural graph of a multilayer perceptron with two hidden layers.}
	 \label{multilayerPerceptron}
 \end{figure}

 The two kinds of signals can be defined as

 \begin{enumerate}
	 \item \textbf{Function Signals:} A function signal is an input (stimulus) that comes in at the input end of the network, propagates forward (neuron by neuron) through the network, and emerges at the output end of the network as an output signal. We refer to such a signal as a "function signal" for two reasons. First, it is presumed to perform a useful function at the output of the network. Second, at each neuron of the network through which a function signal passes, the signal is calculated as a function of the inputs and associated weights applied to that neuron. The function signal is also referred to as the input signal.
	 \item \textbf{Error Signals:} An error signal originates at an output neuron of the network and propagates backward (layer by layer) through the network. We refer to it as an "error signal" because its computation by every neuron of the network involves an error-dependent function in one form or another.
 \end{enumerate}

 Each hidden neuron of a multilayer perceptron is designed to perform two computations:

 \begin{enumerate}
	 \item The computation of the function signal appearing at the output of each neuron, which is expressed as a continuous nonlinear function of the input signal and synaptic weights associated with that neuron;
	 \item The computation of an estimate of the gradient vector (i.e., the gradients of the error surface with respect to the weights connected to the inputs of a neuron), which is needed for the backward pass through the network.
 \end{enumerate}

 The hidden neurons act as \textit{feature detectors}. As the learning process progresses across the multilayer perceptron, the hidden neurons begin to gradually "discover" the salient features that characterize the training data. They do so by performing a nonlinear transformation on the input data into a new space called the \textit{feature space}. In this new space, the classes of interet in a pattern-classification task, for example, may be more easily separated from each other than could be the case in the original input data space.

 \subsection{Batch Learning and On-line Learning}

	 Let $ \mathcal{T} = \{ x(n), d(n) \}_{n = 1}^{N} $ denote the \textit{training sample} used to train the network in a supervised manner. Let $ y_{j}(n) $ denote the function signal produced at the output of neuron $ j $ in the output layer by the stimulus $ x(n) $ applied to the input layer. Correspondingly, the \textit{error signal} produced at the output of neuron $ j $ is defined by $ e_{j}(n) = d_{j} (n) - y_{j} (n) $, where $ d_{j} (n) $ is the $ i $th element of the desired response vector $ d(n) $. The \textit{instantaneous error energy} of neuron $ j $ is defined by

	 $$ \mathcal{E}_{j} (n) = \dfrac{1}{2} e^{2}_{j} (n) $$

	 Summing the error-energy contributions of all the neurons in the output layer, we express the \textit{total instantaneous error energy} of the whole network as

	 $$ \begin{aligned}[t]
			 \mathcal{E} (n) & = \sum_{j \in C} \mathcal{E}_{j} (n)       \\
			                 & = \dfrac{1}{2}\sum_{j \in C} e^{2}_{j} (n) \\
		 \end{aligned} $$

	 \noindent where the set $ C $ includes all the neurons in the output layer. With the training sample consisting of $ N $ examples, the \textit{error energy averaged over the training sample}, or the \textit{empirical risk}, is defined by

	 $$ \begin{aligned}[t]
			 \mathcal{E}_{av} (N) & = \dfrac{1}{N} \sum_{n = 1}^{N} \mathcal{E} (n)             \\
			                      & = \dfrac{1}{2N} \sum_{n = 1}^{N}\sum_{j \in C}e_{j}^{2} (n) \\
		 \end{aligned} $$

	 Depending on how the supervised learning of the multilayer perceptron is actually performed, we may identify two different methods: batch learning and on-line learning.

	 \subsubsection{Batch Learning}
		 In the batch method of supervised learning, adjustments to the synaptic weights of the multilayer perceptron are performed \textit{after} the presentation of \textit{all} the \textit{N} examples in the training sample \( \mathcal{T} \) that constitute one \textit{epoch} of training. In other words, the cost function for batch learning is defined by the average error energy $ \mathcal{E}_{av} $. One realization of the learning curve is obtained by plotting $ \mathcal{E}_{av} $ versus the number of epochs, where, for each epoch of training, the examples in the training sample \( \mathcal{T} \) are \textit{randomly shuffled}. The learning curve is then computed by \textit{ensemble averaging} a large enough number of such realizations where each realization is performed for a \textit{different} set of \textit{initial conditions} chosen at random.

		 \begin{enumerate}
			 \item What \textit{"ensemble averaging"} means?
			 \item What are these \textit{"initial conditions"}?
		 \end{enumerate}

		 With the method of gradient descent used to perform the training, the advantages of batch learning include the following:

		 \begin{itemize}
			 \item \textit{accurate estimation} of the gradient vector. (How?)
			 \item \textit{parallelization} of the learning process. (How?)
		 \end{itemize}

		 In the statistical context, batch learning may be viewed as a form of \textit{statistical inference} (because we are making an estimation of the error function?).

	 \subsubsection{On-line Learning}
		 In the on-line method of supervised learning, adjustments to the synaptic weights of the multilayer perceptron are performed on an \textit{example-by-example basis}. The cost function to be minimized is therefore the total instantaneous error energy $ \mathcal{E} (n) $.
		 Given that the training examples are presented to the network in a random manner, the use of on-line leraning makes the search in the multidimensional weight space \textit{stochastic} in nature; it is for this reason that the method of on-line learning is sometimes referred to as a \textit{stochastic method}. This stochasticity has the desirable effect of making it less likely for the learning process to be trapped in a local minimum.
		 On-line learning is highly popular for solving \textit{pattern-classification problems} for two important practical reasons:

		 \begin{itemize}
			 \item On-line learning is simple to implement.
			 \item It provides effective solutions to large-scale and difficult pattern-classification problems.
		 \end{itemize}

		 (What are the beneficits of batch learning in terms of applicability?)

 \subsection{The Back-Propagation Algorithm}

\end{document}


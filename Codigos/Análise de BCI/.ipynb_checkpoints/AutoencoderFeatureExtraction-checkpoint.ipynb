{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Patrick\\\\Documents\\\\GitHub\\\\ICDeepLearning\\\\Scripts')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "from utils import path\n",
    "from DataVis import createInputFigure\n",
    "from NNUtils import *\n",
    "from NNCreationAux import *\n",
    "from SSVEPDataset import SSVEPDataset\n",
    "from SSVEPDataloader import *\n",
    "from classes import DenseBlock\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataframe):\n",
    "    return torch.tensor(pd.read_csv(dataframe['path'], sep = ' ', header = None, dtype = float).values)\n",
    "\n",
    "def dataVisualization(X):\n",
    "    fig = plt.figure(figsize = (15, 15))\n",
    "    fig.tight_layout()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.imshow(X, cmap = 'gray')\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataType = '512'\n",
    "data = pd.read_csv('SSVEPDataset_'+dataType+'.csv')\n",
    "dataset = SSVEPDataset(data, int(dataType))\n",
    "\n",
    "batchSize = 64\n",
    "dataloaders, datasetsSizes = SSVEPDataloaders(dataset, batchSize)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        n1 = 5\n",
    "        s = 8\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size = (1, 2*n1 + 1), padding = (0, n1), stride = (1, s)),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "        \n",
    "        self.pool    = nn.MaxPool2d(kernel_size = (1, 2), stride = (1, 2), return_indices = True)\n",
    "        self.unpool  = nn.MaxUnpool2d(kernel_size = (1, 2), stride = (1, 2))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 1, kernel_size = (1, 2*n1 + 1), padding = (0, n1), stride = (1, s), output_padding = (0, s - 1))\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x, _2 = self.pool(x)\n",
    "        x = self.unpool(x, _2)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            x = self.encoder(x)\n",
    "            x, _ = self.pool(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16x256_train': [0.7925231569197588,\n",
       "  0.371818972157908,\n",
       "  0.2281032620550512,\n",
       "  0.16802773572804727,\n",
       "  0.13882697272650052,\n",
       "  0.11924531567336875,\n",
       "  0.10441035930162822,\n",
       "  0.09288309383523333,\n",
       "  0.08382104571922358,\n",
       "  0.07663608427012797,\n",
       "  0.07085199233813164,\n",
       "  0.0661369953372758,\n",
       "  0.06225226633933001,\n",
       "  0.05901184170455723,\n",
       "  0.05628689220209261,\n",
       "  0.05396978117716618,\n",
       "  0.051971374795987055,\n",
       "  0.050257127513706465,\n",
       "  0.048770915334805466,\n",
       "  0.047463403844134715,\n",
       "  0.0463110357386507,\n",
       "  0.04529784771276044,\n",
       "  0.0443946500701127,\n",
       "  0.04358127071853086,\n",
       "  0.04285185407478731,\n",
       "  0.04218933436569277,\n",
       "  0.04157797350879117,\n",
       "  0.0410263031011536,\n",
       "  0.04050720649542826,\n",
       "  0.040024620586590016,\n",
       "  0.03957105346105911,\n",
       "  0.039138210589414114,\n",
       "  0.03872863132329214,\n",
       "  0.03833794135313768,\n",
       "  0.037955713318490285,\n",
       "  0.037585160384575524,\n",
       "  0.037219569591921325,\n",
       "  0.03686713724782616,\n",
       "  0.03652794709627008,\n",
       "  0.03619441739368788,\n",
       "  0.03586450972399869,\n",
       "  0.035530798437394505,\n",
       "  0.03521251465593066,\n",
       "  0.03490095075233515,\n",
       "  0.03458843598544816,\n",
       "  0.03428256099586522,\n",
       "  0.03398398948567254,\n",
       "  0.033685698914222226,\n",
       "  0.03338250830332875,\n",
       "  0.03309012800728008,\n",
       "  0.03279721933406788,\n",
       "  0.032509204696167956,\n",
       "  0.03221642160481149,\n",
       "  0.031933872281631706,\n",
       "  0.03164365793486218,\n",
       "  0.03135834521726592,\n",
       "  0.031072120376668132,\n",
       "  0.030787234286685568,\n",
       "  0.030510281659526267,\n",
       "  0.030227257914486386,\n",
       "  0.029946970825011913,\n",
       "  0.029664302729206643,\n",
       "  0.029382032120511645,\n",
       "  0.02909602521621919,\n",
       "  0.02880792824667452,\n",
       "  0.028527535921160554,\n",
       "  0.028246267688470883,\n",
       "  0.02795570875917162,\n",
       "  0.02768414035685115,\n",
       "  0.02739474364307337,\n",
       "  0.027103611865104772,\n",
       "  0.026817897109539954,\n",
       "  0.026533510556424056,\n",
       "  0.026240811870866642,\n",
       "  0.02595850061147641,\n",
       "  0.025670718217452804,\n",
       "  0.025382782090387065,\n",
       "  0.025099452093734848,\n",
       "  0.024819397391417086,\n",
       "  0.024543919269636873,\n",
       "  0.02425573948402326,\n",
       "  0.023983861411829573,\n",
       "  0.02370816927007485,\n",
       "  0.023420480840699576,\n",
       "  0.023145812171282785,\n",
       "  0.022878052066370246,\n",
       "  0.022603333037305665,\n",
       "  0.022354200461899842,\n",
       "  0.022094347469863437,\n",
       "  0.021839587283985957,\n",
       "  0.021605400307165398,\n",
       "  0.021382049271911927,\n",
       "  0.021126648206468465,\n",
       "  0.020911037171389157,\n",
       "  0.020685823593329596,\n",
       "  0.020481306254427074,\n",
       "  0.02026160493929744,\n",
       "  0.020058162148782623,\n",
       "  0.019876458856103184,\n",
       "  0.0196821943212014],\n",
       " '16x256_test': [0.4727771259271182,\n",
       "  0.2744862109750182,\n",
       "  0.18574133045944102,\n",
       "  0.14833167020654503,\n",
       "  0.12598230812575792,\n",
       "  0.10943535174969789,\n",
       "  0.09663904463742679,\n",
       "  0.08664666421902485,\n",
       "  0.07872679324704648,\n",
       "  0.07241587883932686,\n",
       "  0.06728980095286072,\n",
       "  0.06309163725965626,\n",
       "  0.05961134153735507,\n",
       "  0.056688806267230066,\n",
       "  0.05421967558808379,\n",
       "  0.05210459711296218,\n",
       "  0.0502907772285816,\n",
       "  0.04872067324516974,\n",
       "  0.04736484303360894,\n",
       "  0.046159893642742554,\n",
       "  0.04509515189753347,\n",
       "  0.04416355149540709,\n",
       "  0.043309074823127125,\n",
       "  0.04256621725011221,\n",
       "  0.04190169804262154,\n",
       "  0.04126172287614791,\n",
       "  0.040688966370218405,\n",
       "  0.04016682553367737,\n",
       "  0.03967751235097319,\n",
       "  0.039212572814780716,\n",
       "  0.0387774374295067,\n",
       "  0.03837567522794336,\n",
       "  0.037975575147203476,\n",
       "  0.037594761522043316,\n",
       "  0.03722641086239954,\n",
       "  0.036863907762281184,\n",
       "  0.03651796391868329,\n",
       "  0.03618384654814507,\n",
       "  0.035849500515740436,\n",
       "  0.03551654665024726,\n",
       "  0.035187780747920165,\n",
       "  0.03487405451116981,\n",
       "  0.03457361672605787,\n",
       "  0.034265692690353254,\n",
       "  0.03395872661373117,\n",
       "  0.03365205359327924,\n",
       "  0.033352948957201325,\n",
       "  0.03305393054863035,\n",
       "  0.03276258303132249,\n",
       "  0.03247032562891642,\n",
       "  0.03218604771645515,\n",
       "  0.03188752002277217,\n",
       "  0.03162031414690035,\n",
       "  0.0313331408787778,\n",
       "  0.031039526877112877,\n",
       "  0.030774012822043766,\n",
       "  0.03048946900845884,\n",
       "  0.030221449409103218,\n",
       "  0.02993809651487913,\n",
       "  0.029676058769717323,\n",
       "  0.029389544502719418,\n",
       "  0.029116444262392793,\n",
       "  0.028823745583658255,\n",
       "  0.028569673061807514,\n",
       "  0.028281567693302482,\n",
       "  0.02798509111989549,\n",
       "  0.02770119785389184,\n",
       "  0.027432812896349055,\n",
       "  0.027145075096643014,\n",
       "  0.026863973655980148,\n",
       "  0.026574168524844743,\n",
       "  0.02629198280637299,\n",
       "  0.026005776790090095,\n",
       "  0.025718969712545583,\n",
       "  0.025423382398682637,\n",
       "  0.02516533652723054,\n",
       "  0.024887432388606526,\n",
       "  0.024602573317212936,\n",
       "  0.024319206504321796,\n",
       "  0.024052807426714635,\n",
       "  0.02374901095807771,\n",
       "  0.02349791716061014,\n",
       "  0.023200324814998622,\n",
       "  0.022945540578483224,\n",
       "  0.02267222268158918,\n",
       "  0.022411792542471554,\n",
       "  0.022154094480769538,\n",
       "  0.021898029604073846,\n",
       "  0.02166598453954026,\n",
       "  0.021425403934313265,\n",
       "  0.021170850331475446,\n",
       "  0.020941010393175013,\n",
       "  0.020703062619809268,\n",
       "  0.020480471264038767,\n",
       "  0.02027006351794952,\n",
       "  0.02006215961042778,\n",
       "  0.019861673738384423,\n",
       "  0.019669560756493402,\n",
       "  0.019481031667618526,\n",
       "  0.019305023934432876],\n",
       " '16x128_train': [1.019505113472432,\n",
       "  0.5787667715069138,\n",
       "  0.4201072191362416,\n",
       "  0.32983530997793314,\n",
       "  0.27324573897616766,\n",
       "  0.23532887403563266,\n",
       "  0.20774853917268607,\n",
       "  0.18674244985475644,\n",
       "  0.17024895843568738,\n",
       "  0.15706024698285392,\n",
       "  0.14637784927319258,\n",
       "  0.1375244113551828,\n",
       "  0.13013477132215603,\n",
       "  0.12387960390511886,\n",
       "  0.11854300012081971,\n",
       "  0.11393702881676811,\n",
       "  0.1098874766147617,\n",
       "  0.10628731207642363,\n",
       "  0.10305993335369305,\n",
       "  0.10008814679833995,\n",
       "  0.09735952072091154,\n",
       "  0.09480712756569132,\n",
       "  0.09242681460284488,\n",
       "  0.0901774063215151,\n",
       "  0.08808019451605968,\n",
       "  0.08608768806680218,\n",
       "  0.0842109291435598,\n",
       "  0.08244722661299583,\n",
       "  0.08080332581595187,\n",
       "  0.07924917214737708,\n",
       "  0.0777888318230381,\n",
       "  0.07642636871163225,\n",
       "  0.07515652767031183,\n",
       "  0.073971253993747,\n",
       "  0.07289641961837426,\n",
       "  0.07188316311810043,\n",
       "  0.07098602926556444,\n",
       "  0.0701487239027198,\n",
       "  0.06939276741756188,\n",
       "  0.06871756037949642,\n",
       "  0.06808864693720262,\n",
       "  0.06752684914367103,\n",
       "  0.06700598740534032,\n",
       "  0.06657600233624707,\n",
       "  0.06612775428391202,\n",
       "  0.0657288887889394,\n",
       "  0.06539619321024025,\n",
       "  0.06507773882958479,\n",
       "  0.06477373535488988,\n",
       "  0.06451597220295079,\n",
       "  0.06427154217884218,\n",
       "  0.06403498947893307,\n",
       "  0.06382918791784035,\n",
       "  0.06363565020345069,\n",
       "  0.06345953307909406,\n",
       "  0.06330485426353448,\n",
       "  0.06311101425480056,\n",
       "  0.0629742226867012,\n",
       "  0.0628373614979751,\n",
       "  0.06270073414285542,\n",
       "  0.06257592641062788,\n",
       "  0.06244766652147412,\n",
       "  0.06235206561101662,\n",
       "  0.06225403815835387,\n",
       "  0.06216791696546278,\n",
       "  0.06205778195089473,\n",
       "  0.06199365709618335,\n",
       "  0.061920525635773445,\n",
       "  0.061843010695862684,\n",
       "  0.06173700498137282,\n",
       "  0.06168424177082467,\n",
       "  0.06163925139895289,\n",
       "  0.06156454440001603,\n",
       "  0.06148653532013352,\n",
       "  0.06144698261668831,\n",
       "  0.061382397408887145,\n",
       "  0.0613437012902328,\n",
       "  0.06132344186524332,\n",
       "  0.061240442124478545,\n",
       "  0.061236596069274805,\n",
       "  0.06118287323486237,\n",
       "  0.0611699898437266,\n",
       "  0.061104426835919476,\n",
       "  0.061074764314261114,\n",
       "  0.06103902877796264,\n",
       "  0.06100239728887876,\n",
       "  0.060963699041487095,\n",
       "  0.06096015732749041,\n",
       "  0.06094820057843631,\n",
       "  0.060892607539128035,\n",
       "  0.06090414128351561,\n",
       "  0.06086162813417204,\n",
       "  0.060857741141712274,\n",
       "  0.06083786699088502,\n",
       "  0.06081784187710329,\n",
       "  0.06081685987420571,\n",
       "  0.060769907311423794,\n",
       "  0.06077210239438347,\n",
       "  0.060738422167606845,\n",
       "  0.0607704704443177],\n",
       " '16x128_test': [0.7065440811517038,\n",
       "  0.4742476003510611,\n",
       "  0.3624226073205689,\n",
       "  0.29415264682018716,\n",
       "  0.24971849146561745,\n",
       "  0.21841143732105855,\n",
       "  0.19460958198749975,\n",
       "  0.1763130092358851,\n",
       "  0.16176713844795368,\n",
       "  0.15004117568552275,\n",
       "  0.14045414306741932,\n",
       "  0.1324618316832043,\n",
       "  0.1257988893505418,\n",
       "  0.12007531149810924,\n",
       "  0.11518970178269641,\n",
       "  0.11095045395059026,\n",
       "  0.10716711656077878,\n",
       "  0.10384908760641957,\n",
       "  0.10075255527055307,\n",
       "  0.09789810954651117,\n",
       "  0.09527469923098882,\n",
       "  0.09280355083636749,\n",
       "  0.09047938154621439,\n",
       "  0.08832229441884673,\n",
       "  0.08628506521820585,\n",
       "  0.0843518004640118,\n",
       "  0.08258846950727505,\n",
       "  0.08089092401139465,\n",
       "  0.07931400106830912,\n",
       "  0.07780969151101269,\n",
       "  0.07640560377975959,\n",
       "  0.07515967004137598,\n",
       "  0.07388140414005671,\n",
       "  0.07275203236075112,\n",
       "  0.07165587355038185,\n",
       "  0.07072581271658014,\n",
       "  0.0698860995607935,\n",
       "  0.06906120896666915,\n",
       "  0.06839308161766101,\n",
       "  0.0677055203980142,\n",
       "  0.06715943546958895,\n",
       "  0.06661580271009124,\n",
       "  0.0660635477730206,\n",
       "  0.06571446490156782,\n",
       "  0.06525172178561871,\n",
       "  0.06488860859757378,\n",
       "  0.06458833382461533,\n",
       "  0.06424645814812664,\n",
       "  0.0639518698602369,\n",
       "  0.06369147021255214,\n",
       "  0.06340754591610842,\n",
       "  0.06314405512351257,\n",
       "  0.06300457154874836,\n",
       "  0.06277938375433723,\n",
       "  0.06263203512290459,\n",
       "  0.06251003877038047,\n",
       "  0.062310415282572584,\n",
       "  0.06217911526997447,\n",
       "  0.062004376352925,\n",
       "  0.061882062491043145,\n",
       "  0.061728457295960125,\n",
       "  0.061619555475292624,\n",
       "  0.061521461144799276,\n",
       "  0.06142623306357817,\n",
       "  0.061305900169161216,\n",
       "  0.06128674321176805,\n",
       "  0.06117970159366017,\n",
       "  0.06105896977932899,\n",
       "  0.06102063416779696,\n",
       "  0.06092026608658361,\n",
       "  0.060850250347084175,\n",
       "  0.06078104145360954,\n",
       "  0.06071582955100161,\n",
       "  0.06068127499995651,\n",
       "  0.06061342642420814,\n",
       "  0.06056026434832877,\n",
       "  0.060496845497534826,\n",
       "  0.06045176513192854,\n",
       "  0.06041210990794849,\n",
       "  0.06038605197991207,\n",
       "  0.06035683269957046,\n",
       "  0.060304151281659855,\n",
       "  0.060262718834938146,\n",
       "  0.060234978180992736,\n",
       "  0.060188214789270916,\n",
       "  0.06017361942937959,\n",
       "  0.06014827486032095,\n",
       "  0.060124548022454476,\n",
       "  0.06013337286673623,\n",
       "  0.060082247050908894,\n",
       "  0.06005199147122247,\n",
       "  0.060030980369983576,\n",
       "  0.06002614471119839,\n",
       "  0.06000900534646852,\n",
       "  0.05999115049402356,\n",
       "  0.05998050287747995,\n",
       "  0.05995622303186755,\n",
       "  0.059933501708529374,\n",
       "  0.059921225239505695,\n",
       "  0.059909362523328694],\n",
       " '16x64_train': [1.0263727890266168,\n",
       "  0.8037534008969317,\n",
       "  0.6864604347354764,\n",
       "  0.6032886262778397,\n",
       "  0.5385145365973532,\n",
       "  0.487441238902864,\n",
       "  0.44705524221881404,\n",
       "  0.4147548095865564,\n",
       "  0.3882541263496483,\n",
       "  0.3657417758043869,\n",
       "  0.34601682359045677,\n",
       "  0.3283997994639498,\n",
       "  0.31247278775050963,\n",
       "  0.2980296044122605,\n",
       "  0.2850703186604566,\n",
       "  0.27361041733196806,\n",
       "  0.26359309534450154,\n",
       "  0.25504509760783267,\n",
       "  0.24782825378707915,\n",
       "  0.2418190055277758,\n",
       "  0.2368002930185297,\n",
       "  0.2326156042871021,\n",
       "  0.22902446592247094,\n",
       "  0.22595279753863157,\n",
       "  0.22329080628824757,\n",
       "  0.22094147207536102,\n",
       "  0.21883591027050228,\n",
       "  0.21690337714694796,\n",
       "  0.2151364809645838,\n",
       "  0.21350962163764478,\n",
       "  0.2119986559008504,\n",
       "  0.21057177561543364,\n",
       "  0.20921291138007964,\n",
       "  0.20789569032279562,\n",
       "  0.20663784815496578,\n",
       "  0.20541628300051987,\n",
       "  0.20422409545807613,\n",
       "  0.20307343483189524,\n",
       "  0.20195540436458237,\n",
       "  0.20084977859542483,\n",
       "  0.1998052104707166,\n",
       "  0.19873724823251313,\n",
       "  0.19766821818692343,\n",
       "  0.19660832309897566,\n",
       "  0.19556211339030075,\n",
       "  0.19452845253350534,\n",
       "  0.1934824893762777,\n",
       "  0.19244902037875555,\n",
       "  0.19139580876180978,\n",
       "  0.1903355055021279,\n",
       "  0.18928563441985694,\n",
       "  0.188209885096812,\n",
       "  0.1871584300156478,\n",
       "  0.18609165250163376,\n",
       "  0.1849945910247691,\n",
       "  0.18389810632655035,\n",
       "  0.18280832977085323,\n",
       "  0.1816973558494023,\n",
       "  0.1806075223636278,\n",
       "  0.17950171330472925,\n",
       "  0.178396130219484,\n",
       "  0.17728516639588954,\n",
       "  0.17620686307931557,\n",
       "  0.17512753529426378,\n",
       "  0.17407215569482182,\n",
       "  0.17303224977774498,\n",
       "  0.1720373448762265,\n",
       "  0.1710535568959547,\n",
       "  0.17011852000222538,\n",
       "  0.16920247810445863,\n",
       "  0.168307119345927,\n",
       "  0.16744010701720968,\n",
       "  0.1666120152115385,\n",
       "  0.16584175126456516,\n",
       "  0.16509092563674563,\n",
       "  0.16439363229405748,\n",
       "  0.16372824670412603,\n",
       "  0.16307230937830258,\n",
       "  0.16246530195295592,\n",
       "  0.1618856912676668,\n",
       "  0.16132956164660472,\n",
       "  0.16080542045198518,\n",
       "  0.16030912150393475,\n",
       "  0.15983688072625535,\n",
       "  0.15938599948044663,\n",
       "  0.15894325246741048,\n",
       "  0.15853376047951834,\n",
       "  0.15813317370938731,\n",
       "  0.15774970051351483,\n",
       "  0.15737839561678987,\n",
       "  0.15702042201936464,\n",
       "  0.15666837657327617,\n",
       "  0.15633459154502813,\n",
       "  0.15601762790819665,\n",
       "  0.1556937520975595,\n",
       "  0.15538111154413048,\n",
       "  0.155058781851779,\n",
       "  0.1547622433403036,\n",
       "  0.1544573975679202,\n",
       "  0.1541569248223916],\n",
       " '16x64_test': [0.8760140391933176,\n",
       "  0.7280155191054711,\n",
       "  0.634031546203208,\n",
       "  0.5622125427364867,\n",
       "  0.5057904563107334,\n",
       "  0.4613612030015324,\n",
       "  0.4258885431639004,\n",
       "  0.3972895630113371,\n",
       "  0.37308441206212445,\n",
       "  0.3523968459267319,\n",
       "  0.33383164475689003,\n",
       "  0.3172940222116617,\n",
       "  0.3022907603354681,\n",
       "  0.2886520632894048,\n",
       "  0.2764795542636634,\n",
       "  0.2657984701486734,\n",
       "  0.2564946791846237,\n",
       "  0.2486397867456024,\n",
       "  0.24206881105026481,\n",
       "  0.23654676242407424,\n",
       "  0.2320437083095858,\n",
       "  0.22820031430040086,\n",
       "  0.22485292181645558,\n",
       "  0.22201089539152363,\n",
       "  0.21953839314726245,\n",
       "  0.2173375761880106,\n",
       "  0.21533676152264242,\n",
       "  0.2135459298725093,\n",
       "  0.2118567012168549,\n",
       "  0.2103094309022575,\n",
       "  0.20880506894527337,\n",
       "  0.20742026348035414,\n",
       "  0.20613415222683232,\n",
       "  0.20484846868576148,\n",
       "  0.20359011261891097,\n",
       "  0.20239159830542275,\n",
       "  0.20124687609218417,\n",
       "  0.20013261938488092,\n",
       "  0.199019030719013,\n",
       "  0.1979281300808484,\n",
       "  0.19688352500344372,\n",
       "  0.19585667905353366,\n",
       "  0.19481151695653195,\n",
       "  0.19376139304576775,\n",
       "  0.1927131760469723,\n",
       "  0.19169700604218703,\n",
       "  0.19062162253446194,\n",
       "  0.18958135882576743,\n",
       "  0.18856293306900904,\n",
       "  0.18750115213813362,\n",
       "  0.1864344012060445,\n",
       "  0.18538172996087826,\n",
       "  0.18427698810895285,\n",
       "  0.18320774595379394,\n",
       "  0.1821025305942738,\n",
       "  0.1810156043086733,\n",
       "  0.17992379595508506,\n",
       "  0.17884846866785825,\n",
       "  0.17773368179579793,\n",
       "  0.1766688556789042,\n",
       "  0.17562597002083566,\n",
       "  0.1745476471089618,\n",
       "  0.17347391561055794,\n",
       "  0.17240146621242985,\n",
       "  0.1713510899644195,\n",
       "  0.17037199497659564,\n",
       "  0.16938950558066804,\n",
       "  0.16842319529790145,\n",
       "  0.16748334341870122,\n",
       "  0.1666234519346293,\n",
       "  0.16579166376765395,\n",
       "  0.16491707739157555,\n",
       "  0.164203901048545,\n",
       "  0.16343184192102034,\n",
       "  0.16272576835565952,\n",
       "  0.16203507300698278,\n",
       "  0.16139129169913002,\n",
       "  0.1607758397366101,\n",
       "  0.1601927521682921,\n",
       "  0.15964697950925583,\n",
       "  0.15913601192362578,\n",
       "  0.15860345626707043,\n",
       "  0.1581747768781124,\n",
       "  0.15772912009950085,\n",
       "  0.15729772062092037,\n",
       "  0.1568594393712697,\n",
       "  0.156514103050197,\n",
       "  0.156096658099702,\n",
       "  0.15575879643033275,\n",
       "  0.15541442409976497,\n",
       "  0.1550500342697451,\n",
       "  0.15471738806137672,\n",
       "  0.15439538592165644,\n",
       "  0.15408003832394387,\n",
       "  0.1537723223586659,\n",
       "  0.15347839247831058,\n",
       "  0.15321182795278318,\n",
       "  0.15289207086676643,\n",
       "  0.15259362964621392,\n",
       "  0.15228984545875382]}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['16x32_train'] = trainLoss\n",
    "losses['16x32_test']  = testLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Epoch 1/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.1848\n",
      "Stage: val\n",
      "Loss: 1.1159\n",
      "--\n",
      "Epoch 2/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.0753\n",
      "Stage: val\n",
      "Loss: 1.0352\n",
      "--\n",
      "Epoch 3/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.0079\n",
      "Stage: val\n",
      "Loss: 0.9815\n",
      "--\n",
      "Epoch 4/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.9611\n",
      "Stage: val\n",
      "Loss: 0.9409\n",
      "--\n",
      "Epoch 5/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.9238\n",
      "Stage: val\n",
      "Loss: 0.9059\n",
      "--\n",
      "Epoch 6/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.8900\n",
      "Stage: val\n",
      "Loss: 0.8724\n",
      "--\n",
      "Epoch 7/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.8569\n",
      "Stage: val\n",
      "Loss: 0.8392\n",
      "--\n",
      "Epoch 8/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.8235\n",
      "Stage: val\n",
      "Loss: 0.8047\n",
      "--\n",
      "Epoch 9/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.7894\n",
      "Stage: val\n",
      "Loss: 0.7700\n",
      "--\n",
      "Epoch 10/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.7547\n",
      "Stage: val\n",
      "Loss: 0.7347\n",
      "--\n",
      "Epoch 11/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.7197\n",
      "Stage: val\n",
      "Loss: 0.6999\n",
      "--\n",
      "Epoch 12/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6848\n",
      "Stage: val\n",
      "Loss: 0.6650\n",
      "--\n",
      "Epoch 13/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6508\n",
      "Stage: val\n",
      "Loss: 0.6314\n",
      "--\n",
      "Epoch 14/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6181\n",
      "Stage: val\n",
      "Loss: 0.5993\n",
      "--\n",
      "Epoch 15/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.5873\n",
      "Stage: val\n",
      "Loss: 0.5696\n",
      "--\n",
      "Epoch 16/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.5588\n",
      "Stage: val\n",
      "Loss: 0.5423\n",
      "--\n",
      "Epoch 17/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.5329\n",
      "Stage: val\n",
      "Loss: 0.5171\n",
      "--\n",
      "Epoch 18/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.5096\n",
      "Stage: val\n",
      "Loss: 0.4957\n",
      "--\n",
      "Epoch 19/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4890\n",
      "Stage: val\n",
      "Loss: 0.4761\n",
      "--\n",
      "Epoch 20/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4709\n",
      "Stage: val\n",
      "Loss: 0.4591\n",
      "--\n",
      "Epoch 21/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4551\n",
      "Stage: val\n",
      "Loss: 0.4445\n",
      "--\n",
      "Epoch 22/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4414\n",
      "Stage: val\n",
      "Loss: 0.4319\n",
      "--\n",
      "Epoch 23/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4296\n",
      "Stage: val\n",
      "Loss: 0.4206\n",
      "--\n",
      "Epoch 24/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4193\n",
      "Stage: val\n",
      "Loss: 0.4111\n",
      "--\n",
      "Epoch 25/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4103\n",
      "Stage: val\n",
      "Loss: 0.4027\n",
      "--\n",
      "Epoch 26/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4025\n",
      "Stage: val\n",
      "Loss: 0.3954\n",
      "--\n",
      "Epoch 27/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3956\n",
      "Stage: val\n",
      "Loss: 0.3889\n",
      "--\n",
      "Epoch 28/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3895\n",
      "Stage: val\n",
      "Loss: 0.3831\n",
      "--\n",
      "Epoch 29/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3841\n",
      "Stage: val\n",
      "Loss: 0.3778\n",
      "--\n",
      "Epoch 30/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3791\n",
      "Stage: val\n",
      "Loss: 0.3731\n",
      "--\n",
      "Epoch 31/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3747\n",
      "Stage: val\n",
      "Loss: 0.3688\n",
      "--\n",
      "Epoch 32/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3706\n",
      "Stage: val\n",
      "Loss: 0.3649\n",
      "--\n",
      "Epoch 33/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3668\n",
      "Stage: val\n",
      "Loss: 0.3613\n",
      "--\n",
      "Epoch 34/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3633\n",
      "Stage: val\n",
      "Loss: 0.3578\n",
      "--\n",
      "Epoch 35/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3601\n",
      "Stage: val\n",
      "Loss: 0.3548\n",
      "--\n",
      "Epoch 36/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3572\n",
      "Stage: val\n",
      "Loss: 0.3518\n",
      "--\n",
      "Epoch 37/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3544\n",
      "Stage: val\n",
      "Loss: 0.3493\n",
      "--\n",
      "Epoch 38/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3518\n",
      "Stage: val\n",
      "Loss: 0.3468\n",
      "--\n",
      "Epoch 39/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3494\n",
      "Stage: val\n",
      "Loss: 0.3445\n",
      "--\n",
      "Epoch 40/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3472\n",
      "Stage: val\n",
      "Loss: 0.3424\n",
      "--\n",
      "Epoch 41/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3452\n",
      "Stage: val\n",
      "Loss: 0.3404\n",
      "--\n",
      "Epoch 42/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3432\n",
      "Stage: val\n",
      "Loss: 0.3385\n",
      "--\n",
      "Epoch 43/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3414\n",
      "Stage: val\n",
      "Loss: 0.3368\n",
      "--\n",
      "Epoch 44/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3397\n",
      "Stage: val\n",
      "Loss: 0.3352\n",
      "--\n",
      "Epoch 45/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3382\n",
      "Stage: val\n",
      "Loss: 0.3336\n",
      "--\n",
      "Epoch 46/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3367\n",
      "Stage: val\n",
      "Loss: 0.3322\n",
      "--\n",
      "Epoch 47/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3353\n",
      "Stage: val\n",
      "Loss: 0.3308\n",
      "--\n",
      "Epoch 48/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3340\n",
      "Stage: val\n",
      "Loss: 0.3296\n",
      "--\n",
      "Epoch 49/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3328\n",
      "Stage: val\n",
      "Loss: 0.3283\n",
      "--\n",
      "Epoch 50/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3316\n",
      "Stage: val\n",
      "Loss: 0.3272\n",
      "--\n",
      "Epoch 51/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3305\n",
      "Stage: val\n",
      "Loss: 0.3262\n",
      "--\n",
      "Epoch 52/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3294\n",
      "Stage: val\n",
      "Loss: 0.3251\n",
      "--\n",
      "Epoch 53/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3285\n",
      "Stage: val\n",
      "Loss: 0.3242\n",
      "--\n",
      "Epoch 54/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3275\n",
      "Stage: val\n",
      "Loss: 0.3233\n",
      "--\n",
      "Epoch 55/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3266\n",
      "Stage: val\n",
      "Loss: 0.3224\n",
      "--\n",
      "Epoch 56/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3258\n",
      "Stage: val\n",
      "Loss: 0.3215\n",
      "--\n",
      "Epoch 57/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3250\n",
      "Stage: val\n",
      "Loss: 0.3208\n",
      "--\n",
      "Epoch 58/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3242\n",
      "Stage: val\n",
      "Loss: 0.3201\n",
      "--\n",
      "Epoch 59/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3235\n",
      "Stage: val\n",
      "Loss: 0.3193\n",
      "--\n",
      "Epoch 60/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3228\n",
      "Stage: val\n",
      "Loss: 0.3187\n",
      "--\n",
      "Epoch 61/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3222\n",
      "Stage: val\n",
      "Loss: 0.3180\n",
      "--\n",
      "Epoch 62/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3215\n",
      "Stage: val\n",
      "Loss: 0.3174\n",
      "--\n",
      "Epoch 63/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3209\n",
      "Stage: val\n",
      "Loss: 0.3168\n",
      "--\n",
      "Epoch 64/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3203\n",
      "Stage: val\n",
      "Loss: 0.3162\n",
      "--\n",
      "Epoch 65/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3197\n",
      "Stage: val\n",
      "Loss: 0.3157\n",
      "--\n",
      "Epoch 66/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3191\n",
      "Stage: val\n",
      "Loss: 0.3152\n",
      "--\n",
      "Epoch 67/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3186\n",
      "Stage: val\n",
      "Loss: 0.3146\n",
      "--\n",
      "Epoch 68/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3181\n",
      "Stage: val\n",
      "Loss: 0.3141\n",
      "--\n",
      "Epoch 69/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3176\n",
      "Stage: val\n",
      "Loss: 0.3136\n",
      "--\n",
      "Epoch 70/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3171\n",
      "Stage: val\n",
      "Loss: 0.3132\n",
      "--\n",
      "Epoch 71/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3167\n",
      "Stage: val\n",
      "Loss: 0.3127\n",
      "--\n",
      "Epoch 72/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3162\n",
      "Stage: val\n",
      "Loss: 0.3122\n",
      "--\n",
      "Epoch 73/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3158\n",
      "Stage: val\n",
      "Loss: 0.3118\n",
      "--\n",
      "Epoch 74/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3153\n",
      "Stage: val\n",
      "Loss: 0.3114\n",
      "--\n",
      "Epoch 75/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3149\n",
      "Stage: val\n",
      "Loss: 0.3110\n",
      "--\n",
      "Epoch 76/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3145\n",
      "Stage: val\n",
      "Loss: 0.3106\n",
      "--\n",
      "Epoch 77/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3141\n",
      "Stage: val\n",
      "Loss: 0.3102\n",
      "--\n",
      "Epoch 78/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3137\n",
      "Stage: val\n",
      "Loss: 0.3098\n",
      "--\n",
      "Epoch 79/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3134\n",
      "Stage: val\n",
      "Loss: 0.3095\n",
      "--\n",
      "Epoch 80/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3130\n",
      "Stage: val\n",
      "Loss: 0.3091\n",
      "--\n",
      "Epoch 81/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3126\n",
      "Stage: val\n",
      "Loss: 0.3088\n",
      "--\n",
      "Epoch 82/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3123\n",
      "Stage: val\n",
      "Loss: 0.3084\n",
      "--\n",
      "Epoch 83/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3119\n",
      "Stage: val\n",
      "Loss: 0.3081\n",
      "--\n",
      "Epoch 84/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3116\n",
      "Stage: val\n",
      "Loss: 0.3078\n",
      "--\n",
      "Epoch 85/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3113\n",
      "Stage: val\n",
      "Loss: 0.3074\n",
      "--\n",
      "Epoch 86/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3110\n",
      "Stage: val\n",
      "Loss: 0.3071\n",
      "--\n",
      "Epoch 87/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3107\n",
      "Stage: val\n",
      "Loss: 0.3068\n",
      "--\n",
      "Epoch 88/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3103\n",
      "Stage: val\n",
      "Loss: 0.3065\n",
      "--\n",
      "Epoch 89/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3101\n",
      "Stage: val\n",
      "Loss: 0.3062\n",
      "--\n",
      "Epoch 90/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3098\n",
      "Stage: val\n",
      "Loss: 0.3059\n",
      "--\n",
      "Epoch 91/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3095\n",
      "Stage: val\n",
      "Loss: 0.3057\n",
      "--\n",
      "Epoch 92/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3092\n",
      "Stage: val\n",
      "Loss: 0.3054\n",
      "--\n",
      "Epoch 93/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3089\n",
      "Stage: val\n",
      "Loss: 0.3051\n",
      "--\n",
      "Epoch 94/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3087\n",
      "Stage: val\n",
      "Loss: 0.3048\n",
      "--\n",
      "Epoch 95/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3084\n",
      "Stage: val\n",
      "Loss: 0.3046\n",
      "--\n",
      "Epoch 96/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3081\n",
      "Stage: val\n",
      "Loss: 0.3043\n",
      "--\n",
      "Epoch 97/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3079\n",
      "Stage: val\n",
      "Loss: 0.3041\n",
      "--\n",
      "Epoch 98/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3076\n",
      "Stage: val\n",
      "Loss: 0.3038\n",
      "--\n",
      "Epoch 99/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3074\n",
      "Stage: val\n",
      "Loss: 0.3036\n",
      "--\n",
      "Epoch 100/100\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3071\n",
      "Stage: val\n",
      "Loss: 0.3034\n",
      "Best validation loss: 0.303378\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "model = model.to(device)\n",
    "model, _, trainLoss, testLoss = train_autoencoder(model, dataloaders, datasetsSizes, criterion, optimizer, scheduler = None, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplo = next(iter(dataloaders['train']))[0][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "output = model(exemplo).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataVisualization(X1, X2):\n",
    "    fig, axs = plt.subplots(nrows = 2, ncols = 1)\n",
    "    \n",
    "    fig.set_size_inches(12, 5)\n",
    "    fig.subplots_adjust(hspace = 0)\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    \n",
    "    axs[0].imshow(X1, cmap = 'gray'); axs[0].set_title(\"Original\")\n",
    "    axs[1].imshow(X2, cmap = 'gray'); axs[1].set_title(\"Autoencoder's Output\")\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = dataVisualization(exemplo.squeeze()[:, 290:390], output.squeeze()[:, 290:390])\n",
    "fig.savefig(\"16x32_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAG0CAYAAAAxT7bLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debSddXkv8GeHkAQSEghDQCaZCUMVeiIqjjhAcUYqMnjFAatSQeEqUEAFwRt7ZRABB2ylKghWrVBKQY84UtETcGAIyihzIjIEIYQQ3vuH7bprWcT1/H70Udf6fP486/2ub/Z+3/3u/T37LBgNwzAEAAAAzSb9sf8BAAAAf+4MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdJqcDexz2Sh1/HHbTcRRV81LZc4+7Q2p41tt9/nPlfRcFZ8s6YmIiEfeXlJzyPX5zHs2iTjpplzmxG3+NV/UYPTTV5T0DBflXj+tFh1W87+nO/jCr6czx++8Uxx56Q/TuXOufmk60+S9NTUR02pq3vrsmp7PXFJS88aflNRERMQ/nVNUNL/mvhBRc18Y7ZTPTJwZMW//XGbI30b+9BX9unvHx2p6rhj9LB+a2DxiXvJDxrBuvqfJOkU9axX1HFnUExGj9+SOn4iI3Hz4reGPez/1jRUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0Gk0DMPwRAeMj4/H+Ph4RETMnz8/bvzNglTB+qvMjduXLkxlNl28Zur4VldvuklJz7axuKQnIiIeW6ek5tZl+cycqRGLkrkNV7kvX9RgwUOrl/SM3Z97/bRavt5YSc8t99+fzqw/fXrc/uCD6dxmS2elM03WramJuLym5uYZNT1P3aqk5qaHSmoiImKTe4qKNqi5L0TU3BcW5N7yIyJi7lMjFt6cy4zNzff8ySu6FK6puRRimwUNL9i5UyMWJj8sjK2c72lS1fOTop71inoiYsGc3PFzI6LhXhJjf9z76R8cVr9rn8tGqdrjtpuIo66al8qcfdobUse32u7znyvpuSo+WdITERGPvL2k5pDr85n3bBJx0k25zInb/Gu+qMHop68o6Rkuyr1+Wi06LPWybnbwhV9PZ47feac48tIfpnPnXP3SdKbJe2tqIqbV1Lz12TU9n7mkpOaNVZ83IuKfzikqml9zX4iouS+MdspnJs6MmLd/LjPkbyN/+or+jmjHx2p6rhj9LB+a2DxiXvJDxlD1G7GaX15HrFXUc2RRT0SM3pM7fiIicvPht4Y/7v3UnwICAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADpNzgbOfta7c4GJOfnM02bkjm/07hiV9EQMRT0RT9nmgZKeO65f3pCaGSdusySZeWVDT97w4aJztNVhJTVziq7tSafnn7fRFhGTTn9pvuyCDfKZBqeeenRJz64nP1zSs8X1JTXxo31WSWe2Pe77cfVRz0llttx2abqn2Ue+UtMzv+b+c0RJS8Tww5b7z0QMP5z3pP9bnhx17+HxWE3N1lv+oKZoeHZDaCJiyF0Li0Y15+j7UXP/ee3w65KeePV7anoiIoanJQNnRwz7NBRNNGSePL6xAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6DQahmHIBO6Jy1MFM2PrWBLXpjKz47Op45uNTq3pGUY1PRHxnZtSp7PZ8zcpqYkda2riiig6R7fUnJ+jN67p+dCj2+dDk86NeGyvfO55V+UzLbasuRYOurLmHJ2yoObxnBv5x/OSiPhGMrPXoRume5qdcGxNz+hNNT3DlTU9n9whn9njsoivPjOXecej+Z4Ww8E1PRExZ/Sxkp5FVZ9LFjXc52ZHxD25yOvn5GtanFP0WeG0Uc37w4E1Nb91VvL43SLiooaefRsyTyLfWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAp9EwDMMTHTA+Ph7j4+MRETF//vx4NB5MFawU02JFPJzKTI5fp45vtmCjmp6xBTU9EfHAsrGSntWmltTENTU1sU0UnaOi83P7lU/4sn7SrP+XLWdo04i4MR/7+bYNXQ2m1lwLtyytuRY22qbm8dwT+cczMyKWJDOzb/1ZuqfZhk+p6VmwVk3P2EM1PYsb7gtrzI24d2Euc0vNayjGbqnpiYifFH0ueXrV55LlDedopYhYkYvcsHK+psVmRZ8VFi+oubbXKXoJRUSkP9rPioj7G3rWbMg8if7gsPpd98TlqYKZsXUsiWtTmdnx2dTxzUan1vQMo5qeiPjOTTUfqJ+/SUlN7FhTE1dE0Tm6peb8HL1xTc+HHt0+H5p0bsRje+Vzz7sqn2mxZc21cFDR+D1lQc3jOTfyj+clEfGNZGavQzdM9zQ74diantGbanqGK2t6PrlDPrPHZRFffWYu845H8z0thoNreiJizuhjJT2Lqj6XLGq4z82OiHtykdfPyde0OKfos8Jpo5r3hwNran7rrOTxu0XERQ09+zZknkT+FBAAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6DQahmHIBP5jlCv4i4mIn83LZX4+JEsavWlS6qG32+nYmp6IiB+cWFJz10v2TGfWPO2o+PWBx6Uyc75/eLqnxWhpzbUwNbYo6VkWNa+hj+6/MJ3Z7wMbxxeO+WU6NzrzO+lMiw1i1ZKevQ77QElPvPTGmp53vTif+dJpEa87MJf5cs1rKCLi0rc8r6Rn58v2LumJovfWiJr7adWjObKoJyLiuKJH9R+jmnN04fJ85h0rRXxiRS5z3ORX5osabBrnl/TceE5JTax4fdWrKGLf5H3h+Gh77R31kjMaUnnbfeOAx/25b6wAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6jYZhGFKJS5INYxGxIJmZkzy+1bFblNQ8+NhRJT0REdP3XFZTtN7b8pkdIuLHucji2+7K9zRYZ591S3piz1klNR/fffeSnncdu18+dP7OEa+8NB37v+fkMy3e+8wlJT3vfNWpJT0vO6+kJu6PUTqza0zExTEvlbnrdW9I97Q65Evn1hSNiu7bQ/4cNdks97EiIiK+FhGvzkVecUO+psUFNTURETE0vI6ajBrOUYMl3708nZn+9K3jwZ9cm8rcN+PYdE+LjXbYq6Qnzqx5PLF/7nnuk722JyKS7w8REWsUXdv3/p4a31gBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQKfRMAzDEx0wPj4e4+PjERExf/78iCXJhukR8WAys3Ly+FZ3XFlSsyLWK+mJiFhpjSc8nU+eldfOZ1aNiIdykeWPLM/3NFh5zaKL7oYfl9QsmjWrpGfOHbPzoc1nRFz/m3Tsrk3zmRbrznispOeX129U0jNr85KaWBEL0plZMTfuj4WpzPIb1kz3tFp3s3tqihb8ZU3PWP4cNblyLJ/ZLCJuyEWu2z5f0+L+mpqIiBhreB01WdBwjhqs2Cr7ATBi0qrT4rGHHk5lHl3pznRPi6mrrlHSE3fXPJ5Ya7uanoiI9LU9NyL5/hAR8eOia3uH31PzB4fVf3NJsnks8s/lnOTxrY7doqTmwceOKumJiJi+57KaovXels/sEBHJXbH4trvyPQ3W2Wfdkp7Ys2bwfHz33Ut63nXsfvnQ+TtHvPLSdOz/npPPtHjvM7O/PWrzzledWtLzsvNKauL+GKUzu8ZEXBzzUpm7XveGdE+rQ750bk3RqOi+PeTPUZPNGn7B97WIeHUu8orkEGt1QU1NREQMDa+jJqOaX8Iu+e7l6cz0p28dD/7k2lTmvhnHpntabLTDXiU9cWbN44n9c89zn+y1PRGRfH+IiFij6Nq+9/fU+FNAAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAECnydnAv7z/uNTxL/jMm+Pb7//HVOY1Y0enjm/1z5Ny/65Wf733QSU9ERFjv/hqSc+C4w/Ih754ZMTfHp+K3PDVo/I9DdaJ60t6zli4Z0nPpnvUXNuXbLZ1OjM2dYdYsNmCdO65X7ounWnxyK1fLul59YyG11CD533/rJKef/nNTvnQs6bH6Ae53CF33pvvafS9M2vu3bcOo5KefWIo6Rntn89MrBkxL5kbouZ5i1hU1BMRo5pzdNPwnpKeTeLkhtREzHzuvFRiy6Ln7fKi1+pJs2s++3x0+SdLeiIi4pjkOXpHRHwif17vHZ6bzrT53uP+1DdWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnSZnA3fv/JTU8Y/OmJLOxEdemju+0UtGbyrpiS9eX9MTEZfHi2qK3tfY87MzUof/27+P2nqSnjWlpCamXDOU9Lxso38o6Ylv7tYQ2jt2+eYP0qlLRhc1dOVNObHmmvv5655W0vPSszYu6dk7/r4tt9tlqeO3X+sVTT0tPjX20ZKefaacWtITj9TUDMv2bggdH8OyI3OZPfM1Tb68TlFRRAw17xErjU4q6Ylv35XP7Dg74orXpyJ3Fd23TxjVnJ+PDtuV9Hw5rirpiYjYc2ky8FhEZDMREWd9ryHUYN/H/7FvrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACg02gYhuGJDhgfH4/x8fGIiJg/f3786ra7UwWrrzMr7lt8fyqz9gb3pI5v9eiCLUt6Jo/dXtITEbEg1i/pGStpibj9/gUlPetPK6mJu6+seebW2qqkJmK1XzSENoqIW9KpJUWv15ljNdfcogU118KcsUdLeiIml7Rc/eB1JT0RERtPz713tZpx+UYlPfGX69T03H5jPrP2+hG/Sr5XPlzzWSE2q3rHi4h4wo9kT5plC0YlPVO3argWVl0/4qHktfBQzbVw160118K6Y1eX9NwT25b0RETMvjUZmBMRixqKVm3ItFjz8X/8B4fV7zrjsH9M9b76Xa+Mr338/FTmgI+cmzq+1X2ji0t6Vh+OLOmJiBjF8SU9Nbf+iKP+vebmf1zRveWfNq555t74vZKaiOfs1hA6JSIOSqcuGV3U0JW3y1BzzX18VHMtvGu4r6QnYvWSlu0XvKKkJyLiU2MXlPQ8e8qpJT3xyIE1PUftnc+84/iITyTfK689J9/T4stV73gRVe+ut4xq7nMbfbvhWtjx+IgrktfCFTXXwgmH1JyfQ4ftSnq+HFeV9ERE7HloMvDuiDi5oWjHhkyLfR//x/4UEAAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQaXI2cMCRb8kFpk+kM+NHvSbX0Wj2Ji8r6bnt+s1KeiIibr7mUyU9fz1lcToz/1lvicN/8A+pzBYX7ZHuaTJ7lZKaNw57l/T8zejTJT37H7plOrP9wdPiyo/lc7t8YZTONDnutpKan776rJKeiP1KWt46eiidOWpiahw3b1kqc+XVK6d7/uR94cA/9r/gSXXMm7+YzhywVsQZydwHNj0n3fOn7rxRzX3uVcOVJT3xT9/OZ7Z9IOLmXG73LYd8T4MLj645P+9Y/YiSnk/cV1LzWyuSxw8NmYj45bd+nQ812HjfNR/3576xAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6DQahmHIBA4/c1aq4G9f/u049YIXpDLzr943dXyrbfc/u6Tn6uW/KOmJiIgLtiqp+cyW09KZV734ojhvfLdU5q3b3ZnuaXHhNseU9Oz+r3uU9MRe29f03D3KZ6ZORCybl4694eDUrarZ509peEwNvvnNmsfz7acuLOk5ZLufpTOrxUvigfhGKjN7xV7pnlY3HHd6Sc9aH8y/HlrMHGp6Wl5BExGR/dcNTU0NbqupiYgYbVhzXxiGHUp6Il7XkNk/Is7MRUZHNPTkDW/7dUnP6FN/VdKzw1d/VNITEXH4Oi9MHf+Sp30yvvHTt6d79lr4rXSmyQGP/2PfWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAp9EwDMMTHTA+Ph7j4+MRETF//vy47e4fpwrWmbVVLL7/56nMBktnp45vddVa95T0bDdsV9ITERH3XVVS86tp+U2++swt4r4l16Uya6/ytHRPi/tXuaOkZ9Z9q5f0xI2r1vQ8bUE+M2luxGML07Ebbx3LdzXYdKOGx9RgyZKax/PA1KUlPXNWyfesFDNjRSxJZX4y1Lw/RERsf+fikp7Jd0wv6VlprKan5RU0NyKyd4WxpqYGj9TUREQs+FnNfWFs7JqSnog1GjJrRsSvc5EF6zX05A1rP1rSM9o491mp1TX3zi3piYhYd3JuC8xcdeNY8tAv0z2zH94qnWmy9uP/+A8Oq991+JmzUr1/+/Jvx6kXvCCVmX/1vqnjW227/9klPVcv/0VJT0REXFBzQX1my2npzKtefFGcN75bKvPW7e5M97S4cJtjSnp2/9c9Snpir+1reu4e5TNTJyKWzUvH3nBw6lbV7POnNDymBt/8Zs3j+fZT8yO2xSHb/SydWS1eEg/EN1KZ2Sv2Sve0uuG400t61vpg/vXQYuZQ09PyCpqIiOy/bmhqanBbTU1ExGjDmvvCMOxQ0hPxuobM/hFxZi4yOqKhJ294W3LwNRp96q9Kenb46o9KeiIiDl/nhanjX/K0T8Y3fvr2dM9eC7+VzjQ54PF/7E8BAQAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOo2EYhkxgu1iYKjg3nhp7xc2pzI/f9P7U8a2+ucepJT27vWLdkp6IiNHzU6ez2fCdUUNqIiLmpRIv+NWJDT15a5z3npKef3n560p6lu7x9yU9q7x7k3zoxRMR47nrICLi1I8/J9/VYJdNvlfSs83nji7piQs/VNOz+8UNoWdGxGWpxIcOPb6hp81WJ76tpOd1G+5X0hO3tNy3G+zW8D50SkQclIuMLsrXtBh2remJiIiLi87RqOazwm4NmVMmIg5KvkVcNNQ8b9/5t7tLen7w7Q+X9Lz4755S0hMRMfbpQ3OB/xURn8v3nH9YzbXwynj815BvrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADpNzgZ2isNTx0+PE9KZlV+1V+r4Vrud/YqSnvHtjijpiYj45/uOrCk6piFzQESckYsc+aHNG4rypnziKSU9w2tWlPQcP+0TJT27nrVNOvP0HafFTxpyG31v9XSmxRknjZf0nHTph0p6PrD7V0p6jvnyvfnQLjtGXHJrKrL6llvnexrd992TSnoeeO4FJT0Xx1DSs+dFCxtST4246OZUYhjNbehpUPO0lbpm/hdKeg65b2k6M2fdV8chh38tlflhXJXuafH8G15c0vPsPfYu6Vl58XNKeiIilt65Q+r4qcvPimV37pvuOTyOTWdavPL3/Nw3VgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACg02gYhuGJDhgfH4/x8fGIiJg/f37cHNenCtaLDeLOuC2Veeq9a6SOb3bv4pKaJeuvVtITEfHo9aOSntlr3JkPrTU34u6FqciSOzbP9zQYbfzLkp4Zdz/hy+1Jc8ektUp6Zq50Xzqz6gabxUO33ZDOrdh8ajrT4oFFc0p6Npw+s6Tn9hn3lvSsf8+j+dDM1SOW5K6hRSsezPc0mjTtoZKe2avVXNv3x2YlPbNjaUNqakQsy0UWrNLQ02Cspua3FpS0LL1zk5KeR1Y8ls5MX2f1eHBx7r6w0gYz0j0tZiy6qaTnsemzS3omTa553iIiHlt8S+r40ZxNYmh4vq/ZaPV0psV28ZTH/fkfHFa/6y3xqlTx0XFCfCgOTWX+4Wt7pY5vdu7JJTXjH35xSU9ExH2vrhlWe+7x4XzogImIM+alIt/40Pn5ngZTPvE3JT3P+8cVJT1HT9u/pGfXmRekM08/4dz4yaH51/i9522azrT41kkHl/Sc9Iya+8IHdv5KSc8xX24YcLu8KuKS81KRj//6R/meRlO3ubykZ+/nblHSc3GcU9KzZ+R+gfZbT42Im3OR0dyGngY1vw/7TzXv4dd85PMlPbfdlx/ZOx346vjhaV9LZWb9n2ene1rsdMp+JT3L5+1d0rPy7OeX9ERELP3E21PHTz3krFh24r7pnr88eY90psU1cfTj/tyfAgIAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnUbDMAypxIOfzjVMe03Ew/+Sivzy5pVyHY32uXNaSc+lG+1X0hMRcci7c6ez1XYXnpjOvDL2ifPj7FTmr+PQdE+L1XY9s6TnlIdPL+k56NHVSnriwD3zmV33iLj4q/ncPp/MZxrMjBeV9Dww+YSSnuHRUUnP3qP8vef4iYgj5+UyX/xRzeOJiHjtM2rup1/5QklNxL5Fz13DtRATEZG8FlpqWgx1l1xdWdGT95XX5jO7fCTiksNymdeeW/O8PbhSzfM2/cyazz6xf/6zXKvrktfchhMRtybvCRERW5S9YB//8fjGCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoNPkdOK+n+eOX/vhdGbj3R/KdTS69MxflPTs9+hQ0hMR8YULr60p2r3hufvYsnjzwbnc/nd9Kd/T4H1XfL6k56AHrirpiZl31fTs9PJ85rkviFjwxXTsln0+lu9qsOTG5SU923/u0pKeOKemZp0Z30hnJk/aKdaZ8cNU5u7rz0r3tHrv8G9FTRsU9RS9F/3vhsycfG4YNfS0qHsLL/ODD95d0rPbx05PZ6YteWvsdslnUpmJE85O97SY976ai24Yai662+OEkp6IiC2Gq5OJTWOL4cZ80ei6fKbF7zlFvrECAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6jYRiGJzpgfHw8xsfHIyJi/vz5EY/cmmuYPCfi0UW5zLWP5Y5v9dSHS2puXHmrkp6IiE1XWVpTdN3ifGbDdSNuvSsVuWn5avmeButu8+uSnlVWPFDSEz9+Wk3PnOsaMhtHLPplOrZsw/XzXQ2mLnvCW+KT5urf1Pxea9vRtSU9t9y8ZTozZ6vpsejnD6Yy6228PN3TatmaK5X0zIgpJT0Rq9bUJD8mRETEnIhIflRIH99qrKgnIiIWlLT85o6a94hVFv0qnZm01Vrx2M/vTmUeXm9quqfF9HVvKukZ7q656JavVVITERFTIvv5dGpELMsXLRjlMy3Gpj3uj//gsPpvbj80V7z2uyN+dXIu8+yHcse3OvMXJTX7zflmSU9ExBe2qfkQFbsnz2lExMeOiDj4/6Qi+9/1onxPg/dd8fmSnm0eKLoWZuYGbLP3vLwh86mIk/4mHbvlxGPzXQ02urHmg/v2l61S0nPlpOeU9Bx8wNfTmUO/s1Oc8PwfpjJHfzr/Qa3V9XvPKul5ZmxQ0hNR9AuX9zZkDoqIU5KZjzb0tKj5Xct/qvlQ+INjal5Hf/Gx09OZad98azz8os+kMtccvkW6p8W89+1T0jN8tuaiu/1NJTUREbFBXJ1MbBoRN+aLRjUjO4bNH/fH/hQQAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwzq1tPcAAAciSURBVAoAAKCTYQUAANBpNAzDkAokCyYiYl4yM/zku8lEo38/u6Rm6RFvLOmJiDjiomeV9Jz8wuyVEBErT0Qsz10N501LXZ7NVn9g7ZKe5x9zd0nPP3+05nn76680XAcvnIj4VvauEHHaHtvkuxo8Pa4u6dn5i/9W0jNaXvP7s2GbpfnQ3BdGLPxWKvIfY3vkexqdfsO1JT1f2HPrkp74ccPrtcH9o/z9Z/pExIPJ28Ksmttc3FRTExERm6Q/ZbUZNZyjFsPxe+VDb/pwxGf/LhX58t99Ot/TYM8zVi/pmX/x5SU9h390VklPREQ8cmnu+I12j7jlwnTNB7Y8LJ1pcUzc+bg/940VAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACg0+RsYIj9koljY4j3pxJXvGdKsqPNp761S03P6MKSnoiIk5edVtIzetHe6czEGbNj3gG53LJrXpjuaTFlxtklPbHNHSU1vxodXdLzscvTt5B4/U6jOKchd/Cbtk1nWpz6ve+X9Nz40OKSnne95ciSnvvWPTCdmXHxs+I3L1+Yyvz7+fl7T6tDn3FOTdGPh5qeqOmZNWzUkLogZg0vTyXmxy0NPXlHxDUlPRFVZyji0O2PKul52SZfSmdOnvLeeHcy94LLnp/uafL5mjP0steOSnqWbvKRkp6IiFUuOCwXWGci4ro3pnuO3armPfyY33Mp+MYKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQaDcMwPNEB4+PjMT4+HhER8+fPj4gbkxXrR8TtqcSDP6/Ze3dvtVpJz8YLHi7piYiIHZeV1Cz4RT4zd+P1Y+Evc9fCjhsvzxc1mLTKeiU9cfcjJTWLb665Dob17kpnZq+9ddzzq2vTuTmLV09nWizeap2SnpUerDlHD66de821espP88/bSlusGSuu+3Uqs2jzpemeVmtMv6ekZ9UYK+mp87OGzOYRcX0qcWf8RUNP3u1Rd82NxdUlPbdeXfOe9/C6d6YzG86aG7fevzCVWW3qRumeFuveVvP+8NAaC0p6pt26QUlPRMSkzW/LBWbMjfhN7jqIiFhw/dbpTIuxsRmP+/M/OKz+u/2S1cdGxPtTiSteOCXZ0eZT39qlpmf085KeiIhYdlNJzehFj6UzE2ccF/MOOCqVWfbp/E25xZS5f1fSE5+9o6Tm9LfkPqC0Wn7E/HTm9e+8LM45/Znp3MGnviadaXHq9w4q6Zk1cV1Jz8RbjizpOXbdA9OZGRe/OX6z6z+mMiecf1W6p9UezzinpGeHSL4N/8lr+ZB7QUS8PJWYH7c09OQdEdeU9EREDLFtSc///oua+8LCI45PZ05+6US8++vzUpkXbHJauqfFe9/3zpKeK187KunZ/N0fKemJiFjlgsNygZ0nIi7NXQcREaOXfz+daTEMOz/uz/0pIAAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQaXI28Mn4q9Txe8Ss+Goy85s3/XPq+FYzY7+SntixpiYiIqaMSmomrXJLQ2hOTFrlI6nIlLl/n+9p8M7YuKTn9Dd/p6Rn7xdcVtJz266PpjOrzRxil4ZcnP2lfKbB335315Ke2PrrJTWnzX9OSc9F5zwvnXnppjPi68nc37xh1XRPqw0Oubum6KyamvhuzfvDa0dDOvORiYjD5uXeV74ybJHuaXF4XFfS81vPLGk54crjSnoO2fv4dGZaRGy9dy7z3rcdmO5pcd7l7yzpmfKM3Up6th/eV9ITEbH8F2ekjl9p2tRYscXm6Z7h/J3TmSeTb6wAAAA6GVYAAACdDCsAAIBOhhUAAEAnwwoAAKCTYQUAANDJsAIAAOhkWAEAAHQyrAAAADoZVgAAAJ0MKwAAgE6GFQAAQCfDCgAAoJNhBQAA0MmwAgAA6GRYAQAAdDKsAAAAOhlWAAAAnQwrAACAToYVAABAJ8MKAACgk2EFAADQybACAADoZFgBAAB0MqwAAAA6jYZhGP7Y/wgAAIA/Z//j31gdfvjh/9MV/JlwLRDhOuD/cy3wX1wL/BfXAhF/vteBPwUEAADoZFgBAAB0WumDH/zgB/+nSzbddNP/6Qr+TLgWiHAd8P+5FvgvrgX+i2uBiD/P68B/vAIAAKCTPwUEAADoZFgBAAB0MqwAAAA6GVYAAACdDCsAAIBO/w9bv1fnRCoXxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = model.encode(exemplo).detach()[0]\n",
    "image = (image.transpose(0, 2).transpose(0, 1).numpy())\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "fig.tight_layout()\n",
    "ax = fig.add_subplot()\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.imshow(image)\n",
    "fig.savefig(\"16x32_encoded.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = model_selection('resnet50', 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            x = self.encoder(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "--\n",
      "Epoch 1/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.9977 - Accuracy: 0.2051\n",
      "Stage: val\n",
      "Loss: 1.8001 - Accuracy: 0.2473\n",
      "--\n",
      "Epoch 2/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8269 - Accuracy: 0.2294\n",
      "Stage: val\n",
      "Loss: 1.8974 - Accuracy: 0.2143\n",
      "--\n",
      "Epoch 3/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7399 - Accuracy: 0.2399\n",
      "Stage: val\n",
      "Loss: 1.5479 - Accuracy: 0.3315\n",
      "--\n",
      "Epoch 4/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7143 - Accuracy: 0.2266\n",
      "Stage: val\n",
      "Loss: 1.7401 - Accuracy: 0.2546\n",
      "--\n",
      "Epoch 5/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6558 - Accuracy: 0.2614\n",
      "Stage: val\n",
      "Loss: 1.6177 - Accuracy: 0.2912\n",
      "--\n",
      "Epoch 6/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6138 - Accuracy: 0.3205\n",
      "Stage: val\n",
      "Loss: 1.6353 - Accuracy: 0.3168\n",
      "--\n",
      "Epoch 7/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6389 - Accuracy: 0.2903\n",
      "Stage: val\n",
      "Loss: 1.4617 - Accuracy: 0.3681\n",
      "--\n",
      "Epoch 8/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5123 - Accuracy: 0.3594\n",
      "Stage: val\n",
      "Loss: 1.5055 - Accuracy: 0.3974\n",
      "--\n",
      "Epoch 9/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.4959 - Accuracy: 0.3636\n",
      "Stage: val\n",
      "Loss: 1.4726 - Accuracy: 0.4249\n",
      "--\n",
      "Epoch 10/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.4077 - Accuracy: 0.4075\n",
      "Stage: val\n",
      "Loss: 1.3039 - Accuracy: 0.4725\n",
      "--\n",
      "Epoch 11/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.3382 - Accuracy: 0.4730\n",
      "Stage: val\n",
      "Loss: 1.1938 - Accuracy: 0.5330\n",
      "--\n",
      "Epoch 12/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.1570 - Accuracy: 0.5604\n",
      "Stage: val\n",
      "Loss: 1.3468 - Accuracy: 0.4799\n",
      "--\n",
      "Epoch 13/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.1588 - Accuracy: 0.5481\n",
      "Stage: val\n",
      "Loss: 1.1826 - Accuracy: 0.5897\n",
      "--\n",
      "Epoch 14/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.2797 - Accuracy: 0.4895\n",
      "Stage: val\n",
      "Loss: 1.7333 - Accuracy: 0.2436\n",
      "--\n",
      "Epoch 15/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7034 - Accuracy: 0.2569\n",
      "Stage: val\n",
      "Loss: 1.7029 - Accuracy: 0.2161\n",
      "--\n",
      "Epoch 16/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6552 - Accuracy: 0.2669\n",
      "Stage: val\n",
      "Loss: 2.1703 - Accuracy: 0.2784\n",
      "--\n",
      "Epoch 17/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6173 - Accuracy: 0.2788\n",
      "Stage: val\n",
      "Loss: 1.8200 - Accuracy: 0.2949\n",
      "--\n",
      "Epoch 18/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6347 - Accuracy: 0.2665\n",
      "Stage: val\n",
      "Loss: 1.6421 - Accuracy: 0.2436\n",
      "--\n",
      "Epoch 19/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6441 - Accuracy: 0.2445\n",
      "Stage: val\n",
      "Loss: 1.5845 - Accuracy: 0.2802\n",
      "--\n",
      "Epoch 20/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6048 - Accuracy: 0.2761\n",
      "Stage: val\n",
      "Loss: 1.6420 - Accuracy: 0.2711\n",
      "--\n",
      "Epoch 21/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6295 - Accuracy: 0.2706\n",
      "Stage: val\n",
      "Loss: 1.6746 - Accuracy: 0.2619\n",
      "--\n",
      "Epoch 22/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5932 - Accuracy: 0.2866\n",
      "Stage: val\n",
      "Loss: 1.7329 - Accuracy: 0.2509\n",
      "--\n",
      "Epoch 23/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6163 - Accuracy: 0.2807\n",
      "Stage: val\n",
      "Loss: 1.5573 - Accuracy: 0.3040\n",
      "--\n",
      "Epoch 24/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5705 - Accuracy: 0.3178\n",
      "Stage: val\n",
      "Loss: 1.7625 - Accuracy: 0.2802\n",
      "--\n",
      "Epoch 25/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5612 - Accuracy: 0.3040\n",
      "Stage: val\n",
      "Loss: 1.9249 - Accuracy: 0.3443\n",
      "Best validation accuracy: 0.589744\n",
      "\n",
      "Fold 1\n",
      "--\n",
      "Epoch 1/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.9861 - Accuracy: 0.2248\n",
      "Stage: val\n",
      "Loss: 1.8640 - Accuracy: 0.2271\n",
      "--\n",
      "Epoch 2/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8404 - Accuracy: 0.2289\n",
      "Stage: val\n",
      "Loss: 1.8091 - Accuracy: 0.1941\n",
      "--\n",
      "Epoch 3/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8294 - Accuracy: 0.2266\n",
      "Stage: val\n",
      "Loss: 1.7793 - Accuracy: 0.1722\n",
      "--\n",
      "Epoch 4/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7969 - Accuracy: 0.2340\n",
      "Stage: val\n",
      "Loss: 1.7786 - Accuracy: 0.2308\n",
      "--\n",
      "Epoch 5/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7240 - Accuracy: 0.2289\n",
      "Stage: val\n",
      "Loss: 1.7934 - Accuracy: 0.2143\n",
      "--\n",
      "Epoch 6/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7192 - Accuracy: 0.2166\n",
      "Stage: val\n",
      "Loss: 1.6539 - Accuracy: 0.2216\n",
      "--\n",
      "Epoch 7/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7030 - Accuracy: 0.2299\n",
      "Stage: val\n",
      "Loss: 1.8532 - Accuracy: 0.2363\n",
      "--\n",
      "Epoch 8/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7012 - Accuracy: 0.2299\n",
      "Stage: val\n",
      "Loss: 2.0343 - Accuracy: 0.2308\n",
      "--\n",
      "Epoch 9/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6614 - Accuracy: 0.2509\n",
      "Stage: val\n",
      "Loss: 1.7063 - Accuracy: 0.2418\n",
      "--\n",
      "Epoch 10/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6593 - Accuracy: 0.2514\n",
      "Stage: val\n",
      "Loss: 1.5466 - Accuracy: 0.2857\n",
      "--\n",
      "Epoch 11/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6757 - Accuracy: 0.2331\n",
      "Stage: val\n",
      "Loss: 1.9939 - Accuracy: 0.2088\n",
      "--\n",
      "Epoch 12/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6982 - Accuracy: 0.2193\n",
      "Stage: val\n",
      "Loss: 1.6313 - Accuracy: 0.2253\n",
      "--\n",
      "Epoch 13/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6853 - Accuracy: 0.2147\n",
      "Stage: val\n",
      "Loss: 1.6947 - Accuracy: 0.2344\n",
      "--\n",
      "Epoch 14/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6994 - Accuracy: 0.2060\n",
      "Stage: val\n",
      "Loss: 1.6697 - Accuracy: 0.2509\n",
      "--\n",
      "Epoch 15/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6899 - Accuracy: 0.2212\n",
      "Stage: val\n",
      "Loss: 1.6087 - Accuracy: 0.2179\n",
      "--\n",
      "Epoch 16/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6580 - Accuracy: 0.2390\n",
      "Stage: val\n",
      "Loss: 1.6887 - Accuracy: 0.2234\n",
      "--\n",
      "Epoch 17/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6669 - Accuracy: 0.2321\n",
      "Stage: val\n",
      "Loss: 1.6239 - Accuracy: 0.2015\n",
      "--\n",
      "Epoch 18/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6566 - Accuracy: 0.2418\n",
      "Stage: val\n",
      "Loss: 1.6850 - Accuracy: 0.2344\n",
      "--\n",
      "Epoch 19/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6462 - Accuracy: 0.2349\n",
      "Stage: val\n",
      "Loss: 1.8239 - Accuracy: 0.2381\n",
      "--\n",
      "Epoch 20/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6540 - Accuracy: 0.2372\n",
      "Stage: val\n",
      "Loss: 1.9526 - Accuracy: 0.2491\n",
      "--\n",
      "Epoch 21/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6275 - Accuracy: 0.2445\n",
      "Stage: val\n",
      "Loss: 1.7321 - Accuracy: 0.2216\n",
      "--\n",
      "Epoch 22/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6492 - Accuracy: 0.2514\n",
      "Stage: val\n",
      "Loss: 1.9351 - Accuracy: 0.2344\n",
      "--\n",
      "Epoch 23/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6264 - Accuracy: 0.2331\n",
      "Stage: val\n",
      "Loss: 1.6425 - Accuracy: 0.2509\n",
      "--\n",
      "Epoch 24/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6327 - Accuracy: 0.2427\n",
      "Stage: val\n",
      "Loss: 1.8088 - Accuracy: 0.2033\n",
      "--\n",
      "Epoch 25/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6116 - Accuracy: 0.2633\n",
      "Stage: val\n",
      "Loss: 1.7464 - Accuracy: 0.2491\n",
      "Best validation accuracy: 0.285714\n",
      "\n",
      "Fold 2\n",
      "--\n",
      "Epoch 1/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.9936 - Accuracy: 0.2051\n",
      "Stage: val\n",
      "Loss: 1.8807 - Accuracy: 0.2527\n",
      "--\n",
      "Epoch 2/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8710 - Accuracy: 0.2372\n",
      "Stage: val\n",
      "Loss: 1.6202 - Accuracy: 0.2564\n",
      "--\n",
      "Epoch 3/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8128 - Accuracy: 0.2271\n",
      "Stage: val\n",
      "Loss: 1.7984 - Accuracy: 0.2326\n",
      "--\n",
      "Epoch 4/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7378 - Accuracy: 0.2550\n",
      "Stage: val\n",
      "Loss: 1.6319 - Accuracy: 0.2527\n",
      "--\n",
      "Epoch 5/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7259 - Accuracy: 0.2564\n",
      "Stage: val\n",
      "Loss: 1.8575 - Accuracy: 0.3168\n",
      "--\n",
      "Epoch 6/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6368 - Accuracy: 0.2990\n",
      "Stage: val\n",
      "Loss: 1.5814 - Accuracy: 0.3150\n",
      "--\n",
      "Epoch 7/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5695 - Accuracy: 0.3599\n",
      "Stage: val\n",
      "Loss: 1.3892 - Accuracy: 0.4212\n",
      "--\n",
      "Epoch 8/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.3988 - Accuracy: 0.4295\n",
      "Stage: val\n",
      "Loss: 1.4102 - Accuracy: 0.5073\n",
      "--\n",
      "Epoch 9/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.1791 - Accuracy: 0.5582\n",
      "Stage: val\n",
      "Loss: 1.3189 - Accuracy: 0.5714\n",
      "--\n",
      "Epoch 10/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.0647 - Accuracy: 0.6094\n",
      "Stage: val\n",
      "Loss: 1.3745 - Accuracy: 0.5934\n",
      "--\n",
      "Epoch 11/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.8161 - Accuracy: 0.7033\n",
      "Stage: val\n",
      "Loss: 1.8157 - Accuracy: 0.7143\n",
      "--\n",
      "Epoch 12/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6376 - Accuracy: 0.7770\n",
      "Stage: val\n",
      "Loss: 0.7737 - Accuracy: 0.7381\n",
      "--\n",
      "Epoch 13/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.5257 - Accuracy: 0.8141\n",
      "Stage: val\n",
      "Loss: 0.8958 - Accuracy: 0.7088\n",
      "--\n",
      "Epoch 14/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4958 - Accuracy: 0.8260\n",
      "Stage: val\n",
      "Loss: 0.6860 - Accuracy: 0.7967\n",
      "--\n",
      "Epoch 15/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.2788 - Accuracy: 0.9061\n",
      "Stage: val\n",
      "Loss: 0.5344 - Accuracy: 0.8278\n",
      "--\n",
      "Epoch 16/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.2293 - Accuracy: 0.9258\n",
      "Stage: val\n",
      "Loss: 0.6330 - Accuracy: 0.8114\n",
      "--\n",
      "Epoch 17/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.1664 - Accuracy: 0.9473\n",
      "Stage: val\n",
      "Loss: 0.4143 - Accuracy: 0.8553\n",
      "--\n",
      "Epoch 18/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.1076 - Accuracy: 0.9734\n",
      "Stage: val\n",
      "Loss: 0.4211 - Accuracy: 0.8553\n",
      "--\n",
      "Epoch 19/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.1171 - Accuracy: 0.9675\n",
      "Stage: val\n",
      "Loss: 0.4113 - Accuracy: 0.8681\n",
      "--\n",
      "Epoch 20/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0718 - Accuracy: 0.9821\n",
      "Stage: val\n",
      "Loss: 0.3695 - Accuracy: 0.8700\n",
      "--\n",
      "Epoch 21/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.1093 - Accuracy: 0.9638\n",
      "Stage: val\n",
      "Loss: 0.4585 - Accuracy: 0.8480\n",
      "--\n",
      "Epoch 22/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0882 - Accuracy: 0.9744\n",
      "Stage: val\n",
      "Loss: 0.4155 - Accuracy: 0.8773\n",
      "--\n",
      "Epoch 23/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0526 - Accuracy: 0.9840\n",
      "Stage: val\n",
      "Loss: 0.3493 - Accuracy: 0.8736\n",
      "--\n",
      "Epoch 24/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0564 - Accuracy: 0.9863\n",
      "Stage: val\n",
      "Loss: 0.3709 - Accuracy: 0.8846\n",
      "--\n",
      "Epoch 25/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0604 - Accuracy: 0.9817\n",
      "Stage: val\n",
      "Loss: 0.4030 - Accuracy: 0.8828\n",
      "Best validation accuracy: 0.884615\n",
      "\n",
      "Fold 3\n",
      "--\n",
      "Epoch 1/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 2.0128 - Accuracy: 0.2042\n",
      "Stage: val\n",
      "Loss: 1.9293 - Accuracy: 0.2179\n",
      "--\n",
      "Epoch 2/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8424 - Accuracy: 0.2198\n",
      "Stage: val\n",
      "Loss: 2.1234 - Accuracy: 0.2271\n",
      "--\n",
      "Epoch 3/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7703 - Accuracy: 0.2102\n",
      "Stage: val\n",
      "Loss: 2.4560 - Accuracy: 0.2198\n",
      "--\n",
      "Epoch 4/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7291 - Accuracy: 0.2143\n",
      "Stage: val\n",
      "Loss: 1.8664 - Accuracy: 0.2106\n",
      "--\n",
      "Epoch 5/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7325 - Accuracy: 0.2303\n",
      "Stage: val\n",
      "Loss: 2.3195 - Accuracy: 0.2033\n",
      "--\n",
      "Epoch 6/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7186 - Accuracy: 0.2042\n",
      "Stage: val\n",
      "Loss: 1.8189 - Accuracy: 0.1868\n",
      "--\n",
      "Epoch 7/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7145 - Accuracy: 0.2216\n",
      "Stage: val\n",
      "Loss: 1.8395 - Accuracy: 0.2381\n",
      "--\n",
      "Epoch 8/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6868 - Accuracy: 0.2207\n",
      "Stage: val\n",
      "Loss: 2.1261 - Accuracy: 0.2308\n",
      "--\n",
      "Epoch 9/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6905 - Accuracy: 0.2280\n",
      "Stage: val\n",
      "Loss: 2.0609 - Accuracy: 0.2308\n",
      "--\n",
      "Epoch 10/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6810 - Accuracy: 0.2395\n",
      "Stage: val\n",
      "Loss: 2.3420 - Accuracy: 0.2381\n",
      "--\n",
      "Epoch 11/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6585 - Accuracy: 0.2335\n",
      "Stage: val\n",
      "Loss: 2.3638 - Accuracy: 0.2234\n",
      "--\n",
      "Epoch 12/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6873 - Accuracy: 0.2189\n",
      "Stage: val\n",
      "Loss: 1.7790 - Accuracy: 0.2253\n",
      "--\n",
      "Epoch 13/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6485 - Accuracy: 0.2376\n",
      "Stage: val\n",
      "Loss: 1.8690 - Accuracy: 0.2271\n",
      "--\n",
      "Epoch 14/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6576 - Accuracy: 0.2495\n",
      "Stage: val\n",
      "Loss: 2.1429 - Accuracy: 0.2821\n",
      "--\n",
      "Epoch 15/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6431 - Accuracy: 0.2505\n",
      "Stage: val\n",
      "Loss: 1.7456 - Accuracy: 0.2234\n",
      "--\n",
      "Epoch 16/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6482 - Accuracy: 0.2454\n",
      "Stage: val\n",
      "Loss: 1.7458 - Accuracy: 0.2381\n",
      "--\n",
      "Epoch 17/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6025 - Accuracy: 0.2532\n",
      "Stage: val\n",
      "Loss: 1.7500 - Accuracy: 0.2692\n",
      "--\n",
      "Epoch 18/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5923 - Accuracy: 0.2766\n",
      "Stage: val\n",
      "Loss: 1.7875 - Accuracy: 0.2619\n",
      "--\n",
      "Epoch 19/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6110 - Accuracy: 0.2656\n",
      "Stage: val\n",
      "Loss: 3.4074 - Accuracy: 0.2454\n",
      "--\n",
      "Epoch 20/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6662 - Accuracy: 0.2248\n",
      "Stage: val\n",
      "Loss: 2.2680 - Accuracy: 0.2161\n",
      "--\n",
      "Epoch 21/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6540 - Accuracy: 0.2271\n",
      "Stage: val\n",
      "Loss: 1.8282 - Accuracy: 0.2381\n",
      "--\n",
      "Epoch 22/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6587 - Accuracy: 0.2578\n",
      "Stage: val\n",
      "Loss: 2.1743 - Accuracy: 0.2253\n",
      "--\n",
      "Epoch 23/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6089 - Accuracy: 0.2651\n",
      "Stage: val\n",
      "Loss: 3.9721 - Accuracy: 0.2344\n",
      "--\n",
      "Epoch 24/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6207 - Accuracy: 0.2596\n",
      "Stage: val\n",
      "Loss: 2.6626 - Accuracy: 0.2454\n",
      "--\n",
      "Epoch 25/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.5896 - Accuracy: 0.2729\n",
      "Stage: val\n",
      "Loss: 2.6852 - Accuracy: 0.2802\n",
      "Best validation accuracy: 0.282051\n",
      "\n",
      "Fold 4\n",
      "--\n",
      "Epoch 1/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 2.0209 - Accuracy: 0.2157\n",
      "Stage: val\n",
      "Loss: 1.7141 - Accuracy: 0.2656\n",
      "--\n",
      "Epoch 2/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8353 - Accuracy: 0.2408\n",
      "Stage: val\n",
      "Loss: 1.9240 - Accuracy: 0.2912\n",
      "--\n",
      "Epoch 3/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.8002 - Accuracy: 0.2422\n",
      "Stage: val\n",
      "Loss: 1.7162 - Accuracy: 0.2601\n",
      "--\n",
      "Epoch 4/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7501 - Accuracy: 0.2523\n",
      "Stage: val\n",
      "Loss: 1.6486 - Accuracy: 0.2491\n",
      "--\n",
      "Epoch 5/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.7110 - Accuracy: 0.2624\n",
      "Stage: val\n",
      "Loss: 1.6733 - Accuracy: 0.2619\n",
      "--\n",
      "Epoch 6/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6269 - Accuracy: 0.3164\n",
      "Stage: val\n",
      "Loss: 1.7078 - Accuracy: 0.2857\n",
      "--\n",
      "Epoch 7/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.6148 - Accuracy: 0.3196\n",
      "Stage: val\n",
      "Loss: 1.4490 - Accuracy: 0.3956\n",
      "--\n",
      "Epoch 8/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.4824 - Accuracy: 0.3906\n",
      "Stage: val\n",
      "Loss: 1.4010 - Accuracy: 0.4103\n",
      "--\n",
      "Epoch 9/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.4108 - Accuracy: 0.4322\n",
      "Stage: val\n",
      "Loss: 1.6134 - Accuracy: 0.4139\n",
      "--\n",
      "Epoch 10/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.3620 - Accuracy: 0.4588\n",
      "Stage: val\n",
      "Loss: 1.5046 - Accuracy: 0.4597\n",
      "--\n",
      "Epoch 11/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 1.2463 - Accuracy: 0.5073\n",
      "Stage: val\n",
      "Loss: 1.1362 - Accuracy: 0.5659\n",
      "--\n",
      "Epoch 12/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.9347 - Accuracy: 0.6502\n",
      "Stage: val\n",
      "Loss: 0.9080 - Accuracy: 0.6575\n",
      "--\n",
      "Epoch 13/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.7977 - Accuracy: 0.7092\n",
      "Stage: val\n",
      "Loss: 0.8082 - Accuracy: 0.7015\n",
      "--\n",
      "Epoch 14/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6786 - Accuracy: 0.7505\n",
      "Stage: val\n",
      "Loss: 1.3764 - Accuracy: 0.5403\n",
      "--\n",
      "Epoch 15/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.8712 - Accuracy: 0.6859\n",
      "Stage: val\n",
      "Loss: 1.0879 - Accuracy: 0.6062\n",
      "--\n",
      "Epoch 16/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.6389 - Accuracy: 0.7743\n",
      "Stage: val\n",
      "Loss: 0.6615 - Accuracy: 0.7619\n",
      "--\n",
      "Epoch 17/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.4225 - Accuracy: 0.8535\n",
      "Stage: val\n",
      "Loss: 0.4903 - Accuracy: 0.8462\n",
      "--\n",
      "Epoch 18/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.3025 - Accuracy: 0.8961\n",
      "Stage: val\n",
      "Loss: 0.3754 - Accuracy: 0.8700\n",
      "--\n",
      "Epoch 19/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.1370 - Accuracy: 0.9551\n",
      "Stage: val\n",
      "Loss: 0.3376 - Accuracy: 0.8883\n",
      "--\n",
      "Epoch 20/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0955 - Accuracy: 0.9716\n",
      "Stage: val\n",
      "Loss: 0.3079 - Accuracy: 0.9194\n",
      "--\n",
      "Epoch 21/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0731 - Accuracy: 0.9789\n",
      "Stage: val\n",
      "Loss: 0.2651 - Accuracy: 0.9231\n",
      "--\n",
      "Epoch 22/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0649 - Accuracy: 0.9826\n",
      "Stage: val\n",
      "Loss: 0.2764 - Accuracy: 0.9139\n",
      "--\n",
      "Epoch 23/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0575 - Accuracy: 0.9844\n",
      "Stage: val\n",
      "Loss: 0.2842 - Accuracy: 0.9084\n",
      "--\n",
      "Epoch 24/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0255 - Accuracy: 0.9945\n",
      "Stage: val\n",
      "Loss: 0.2628 - Accuracy: 0.9212\n",
      "--\n",
      "Epoch 25/25\n",
      "-----\n",
      "Stage: train\n",
      "Loss: 0.0155 - Accuracy: 0.9973\n",
      "Stage: val\n",
      "Loss: 0.2360 - Accuracy: 0.9359\n",
      "Best validation accuracy: 0.935897\n",
      "\n",
      "Training complete in 121m 45s\n",
      "Mean acc: 0.5956043956043956\n"
     ]
    }
   ],
   "source": [
    "hybrid = HybridModel(model.encoder)\n",
    "hybrid = hybrid.to(device)\n",
    "optimizer = optim.SGD(hybrid.parameters(), lr = 0.001, momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "hybrid, _, _ = kfold_train(hybrid, dataset, criterion, optimizer, num_epochs = 25, num_folds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloaders, dataset_size, criterion, optimizer, scheduler = None, num_epochs = 10):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wgts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "    trainLoss = []\n",
    "    testLoss  = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-'*2)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-'*5)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            print('Stage: {}'.format(phase))\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            cumulative_loss = 0.0\n",
    "            cumulative_hits  = 0\n",
    "            \n",
    "            for inputs, _ in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                cumulative_loss += loss.item()*inputs.size(0)\n",
    "                del(inputs); del(_)\n",
    "                \n",
    "            if phase == 'train' and scheduler != None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = cumulative_loss / dataset_size[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                trainLoss.append(epoch_loss)\n",
    "            else:\n",
    "                testLoss.append(epoch_loss)\n",
    "                \n",
    "            print('Loss: {:.4f}'.format(epoch_loss))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wgts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Best validation loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wgts)\n",
    "    return model, time_elapsed, trainLoss, testLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, neuronsPerLayer):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            neuronsPerLayer: a list of integers [x0, x1, ..., xn] where xi is the\n",
    "                number of neurons at layer i. Layer 0 is the input layer, and\n",
    "                layer n is the output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # implement that using a design pattern to work with more than one\n",
    "        # activation function. The user should be able to select it.\n",
    "        self.activationFunction = self.tanh\n",
    "        self.activationDerivative = self.dtanh\n",
    "        \n",
    "        self.nLayers = len(neuronsPerLayer)\n",
    "        self.neuronsPerLayer = neuronsPerLayer\n",
    "        \n",
    "        self.stepsPerEpoch = 1000\n",
    "        \n",
    "        # initialize the weights with random values in the range (-1, 1)\n",
    "        self.W = []\n",
    "        for layer in range(1, self.nLayers):\n",
    "            W = 2*np.random.rand(self.neuronsPerLayer[layer],              # number of neurons\n",
    "                                 self.neuronsPerLayer[layer - 1] + 1) - 1  # output: number of neurons on the previous layer + bias\n",
    "            self.W.append(W)\n",
    "            \n",
    "            \n",
    "    def fit(self, data, learningRate = 0.1, epochs = 10):\n",
    "        \"\"\"\n",
    "        \n",
    "        input:\n",
    "            data:\n",
    "            \n",
    "            learningRate:\n",
    "            \n",
    "            epochs:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add bias units to the input layer\n",
    "        ones = numpy.ones((data.shape[0], 1))\n",
    "        X = np.concatenate([ones, data], axis = 1)\n",
    "        \n",
    "        traningSet = X[np.random.randint(low = 0, high = data.shape[0], size = int(data.shape[0]*0.8))] \n",
    "        index = numpy.arange(len(trainingSet))\n",
    "#         validationSet = \n",
    "        \n",
    "        for k in range(epochs):\n",
    "            # in each epoch, the network will process the data randomly reordered.\n",
    "            np.random.shuffle(index)\n",
    "            for i in index: # run through the training sample\n",
    "                Y = [trainingSet[i][:-1].T] # sequence of outputs; starts with the output\n",
    "                                            # from the input layer\n",
    "                # forward\n",
    "                for j in range(self.nLayers - 1): # run through the layers\n",
    "                    localField = self.W[j].dot(Y[j])\n",
    "                    output = self.tanh(localField)\n",
    "                    nextInput = np.concatenate([np.ones((1, 1)), output])\n",
    "                    Y.append(nextInput)\n",
    "                \n",
    "                # output layer\n",
    "                localField = self.W[-1].dot(Y[-1])\n",
    "                output = self.tanh(localField)\n",
    "                Y.append(output)\n",
    "                \n",
    "                # backward\n",
    "                error = traningSet[i][-1] - Y[-1] # error for the output layer\n",
    "                deltaa = [error*self.dtanh(Y[-1])]\n",
    "                # computes all the deltas\n",
    "                for j in range(self.nLayers - 2, 0, -1):\n",
    "                    error = deltas[-1].dot(self.W[j][1:].T)\n",
    "                    error = error*self.dtanh(y[i][1:])\n",
    "                    deltas.append(error)\n",
    "                    \n",
    "                deltas.reverse()\n",
    "                # backpropagation\n",
    "                for j in range(self.nLayers):\n",
    "                    layerOutput = Y[i].reshape(1, self.neuronsPerLayer[i] + 1)\n",
    "                    delta = deltas[i].reshape(1, self.neuronsPerLayer[i+1])\n",
    "                    self.W[i] += learning_rate * layer.T.dot(delta)\n",
    "                    \n",
    "                    \n",
    "    def predict(self, X):\n",
    "        \n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))\n",
    "    \n",
    "    def dtanh(self, x):\n",
    "        return (1.0 + self.tanh(x))*(1 - self.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, neuronsPerLayer):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            neuronsPerLayer: a list of integers [x0, x1, ..., xn] where xi is the\n",
    "                number of neurons at layer i. Layer 0 is the input layer, and\n",
    "                layer n is the output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # implement that using a design pattern to work with more than one\n",
    "        # activation function. The user should be able to select it.\n",
    "        self.activationFunction = self.tanh\n",
    "        self.activationDerivative = self.dtanh\n",
    "        \n",
    "        self.nLayers = len(neuronsPerLayer)\n",
    "        self.neuronsPerLayer = neuronsPerLayer\n",
    "        \n",
    "        self.stepsPerEpoch = 1000\n",
    "        \n",
    "        # initialize the weights with random values in the range (-1, 1)\n",
    "        self.W = []\n",
    "        for layer in range(1, self.nLayers):\n",
    "            W = 2*np.random.rand(self.neuronsPerLayer[layer],              # number of neurons\n",
    "                                 self.neuronsPerLayer[layer - 1] + 1) - 1  # output: number of neurons on the previous layer + bias\n",
    "            self.W.append(W)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self, data, target, learningRate = 0.1, epochs = 5):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        input:\n",
    "            data: An nxm array, where n is the sample size, m is the number of features, and the last column is the target value.\n",
    "                           [x11 ... x1m]\n",
    "                    data = [... ... ...]\n",
    "                           [xn1 ... xnm]\n",
    "                  It is assumed that m equals the size of the input layer.\n",
    "                  \n",
    "            target\n",
    "            \n",
    "            learningRate:\n",
    "            \n",
    "            epochs:\n",
    "        \"\"\"\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        # Add bias units to the input layer\n",
    "        ones = numpy.ones((data.shape[0], 1))\n",
    "        X = np.concatenate([ones, data], axis = 1)\n",
    "        # randomly select a training set\n",
    "        index = np.random.randint(low = 0, high = data.shape[0], size = int(data.shape[0]*0.8))\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            # shuffle the training set in each epoch\n",
    "            np.random.shuffle(index)\n",
    "            # forward\n",
    "            self.forward(trainingSet[index], target[index])\n",
    "            # backward\n",
    "            \n",
    "            \n",
    "                \n",
    "    \n",
    "    def forward(self, X, target):\n",
    "        self.Y = [X.T]\n",
    "        self.V = [X.T]\n",
    "        for i in range(self.nLayers - 1):\n",
    "            localField = self.W.dot(self.Y[-1])\n",
    "            output = self.tanh(localField)\n",
    "            output = np.concatenate([np.ones((1, output.shape[1]))])\n",
    "            self.Y.append(output)\n",
    "            self.V.append(localField)\n",
    "        # last layer\n",
    "        localField = self.W.dot(self.Y[-1])\n",
    "        output = self.tanh(localField)\n",
    "        self.Y.append(output)\n",
    "        self.V.append(localField)\n",
    "    \n",
    "    def backward(self, X, target):\n",
    "        outputError = np.mean(target - self.Y[-1]).reshape((self.neuronsPerLayer[-1], 1))\n",
    "        delta = outputError*self.dtanh(self.Y[-1])\n",
    "        for i in range(self.nLayers - 1, 0, -1):\n",
    "            self.W[i] += self.learningRate*delta*self.Y[i - 1]\n",
    "            delta = self.dtanh(Y[i-1])*delta.T.dot(self.W[i])\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))\n",
    "    \n",
    "    def dtanh(self, x):\n",
    "        return (1.0 + self.tanh(x))*(1 - self.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.random.rand(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18038689, 0.17092521, 0.35656005, 0.97897985, 0.80239463],\n",
       "       [0.2440659 , 0.10224412, 0.80721809, 0.89696719, 0.40853055],\n",
       "       [0.65545617, 0.00863911, 0.46531992, 0.63003833, 0.02649213]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.asarray(([0], [0], [1]))\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49784933, -0.49180517,  0.64281087])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(target - output, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
<<<<<<< HEAD
 "cells": [],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('C:\\\\Users\\\\Patrick\\\\Documents\\\\GitHub\\\\ICDeepLearning\\\\Scripts')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from torchvision import datasets, transforms\n",
    "from classes import *\n",
    "\n",
    "mnist_data = datasets.MNIST('data', train = True, download = True,\n",
    "                           transform = transforms.ToTensor())\n",
    "mnist_data = list(mnist_data)[:4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders with convolution and deconvolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock(1, 16, kernel_size = 3, stride = 2, padding = 1),\n",
    "            ConvBlock(16, 32, kernel_size = 3, stride = 2, padding = 1),\n",
    "            ConvBlock(32, 64, kernel_size = 7, activFunc = 'none')\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            DeconvBlock(64, 32, kernel_size = 7),\n",
    "            DeconvBlock(32, 16, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),\n",
    "            DeconvBlock(16, 1,  kernel_size = 3, stride = 2, padding = 1, output_padding = 1, activFunc = 'sigmoid')\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.decoder(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs = 5, batch_size = 64, learning_rate = 1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr = learning_rate,\n",
    "                                 weight_decay = 1e-5)\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    outputs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            recon = model(img)\n",
    "            loss = criterion(recon, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        outputs.append((epoch, img, recon),)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().\n",
    "max_epochs = 20\n",
    "outputs = train(model, num_epochs = max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, max_epochs, 5):\n",
    "    plt.figure(figsize = (9, 2))\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1)\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder with upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or oher kinds of reconstruction and transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWorkers = 2\n",
    "batchSize = 20\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root = 'data', train = True,\n",
    "                            download = True, transform = transform)\n",
    "test_data  = datasets.MNIST(root = 'data', train = False,\n",
    "                            download = True, transform = transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batchSize,\n",
    "                                          num_workers = numWorkers)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size = batchSize,\n",
    "                                          num_workers = numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "images, labels = iter(trainLoader).next()\n",
    "images = images.numpy()\n",
    "\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # encoder layers #\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # decoder layers #\n",
    "        self.conv4 = nn.Conv2d(4, 16, 3, padding = 1)\n",
    "        self.conv5 = nn.Conv2d(16, 1, 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder #\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # decoder #\n",
    "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = ConvAutoencoder()\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data in trainLoader:\n",
    "        images, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainLoader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testLoader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "output = model(images)\n",
    "images = images.numpy()\n",
    "\n",
    "output = output.view(batchSize, 1, 28, 28)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 10, sharex = True, sharey = True, figsize = (25, 4))\n",
    "\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap = 'gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.\n",
    "\n",
    "* We'll use noisy images as input and the original, clean images as targets.\n",
    "\n",
    "* Try with different types of noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvDenoiser, self).__init__()\n",
    "        \n",
    "        # encoder #\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # decoder #\n",
    "        self.t_conv1 = nn.ConvTranspose2d(8, 8, 3, stride = 2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(8, 16, 2, stride = 2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(16, 32, 2, stride = 2)\n",
    "        self.conv_out = nn.Conv2d(32, 1, 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encode #\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # decode #\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        x = F.sigmoid(self.conv_out(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = ConvDenoiser()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "n_epochs = 20\n",
    "noise_factor = 0.5\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data in trainLoader:\n",
    "        images, _ = data\n",
    "        \n",
    "        noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(noisy_imgs)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainLoader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        #encoder\n",
    "        self.fc1 = nn.Linear(28*28, encoding_dim)\n",
    "        \n",
    "        #decoder\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # decoding\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def getLoss(network, train_loader):\n",
    "    '''\n",
    "        network:\n",
    "        dataset: test dataset\n",
    "        \n",
    "        Network will be applied to a test dataset, returning the resultant loss.\n",
    "    '''\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for data in train_loader:\n",
    "            images, _ = data\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            train_loss += loss.item()*images.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss\n",
    "    \n",
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6279821547865865"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLoss(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Genetic Algorithm Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        #encoder\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        \n",
    "        #decoder\n",
    "        self.fc2 = nn.Linear(32, 28*28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # decoding\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWorkers = 2\n",
    "batchSize = 20\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root = 'data', train = True,\n",
    "                            download = True, transform = transform)\n",
    "test_data  = datasets.MNIST(root = 'data', train = False,\n",
    "                            download = True, transform = transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batchSize,\n",
    "                                          num_workers = numWorkers)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size = batchSize,\n",
    "                                          num_workers = numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndividual():\n",
    "    network = Autoencoder()\n",
    "    return network,\n",
    "\n",
    "def getLoss(individual):\n",
    "    '''\n",
    "        dataset: test dataset\n",
    "        \n",
    "        Network will be applied to a test dataset, returning the resultant loss.\n",
    "    '''\n",
    "    train_loss = 0.0\n",
    "    network = individual[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            images, _ = data\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            train_loss += loss.item()*images.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss,\n",
    "    \n",
    "def getWeights(individual):\n",
    "    model = individual[0]\n",
    "    encoderWeights = {\n",
    "        'weights': model.state_dict()['fc1.weight'],\n",
    "        'bias': model.state_dict()['fc1.bias'],\n",
    "    }\n",
    "    decoderWeights = {\n",
    "        'weights': model.state_dict()['fc2.weight'],\n",
    "        'bias': model.state_dict()['fc2.bias'],\n",
    "    }\n",
    "    return encoderWeights, decoderWeights\n",
    "\n",
    "def setWeights(model, encoderWeights, decoderWeights):\n",
    "    model = copy.deepcopy(model) # preciso disso mesmo?\n",
    "    with torch.no_grad():\n",
    "        model.fc1.weights = encoderWeights['weights']\n",
    "        model.fc1.bias = encoderWeights['bias']\n",
    "        model.fc2.weights = decoderWeights['weights']\n",
    "        model.fc2.bias = decoderWeights['bias']\n",
    "    return model\n",
    "    \n",
    "def simulatedBinaryCrossover(f1, f2, beta):\n",
    "    off1 = ((1+beta)*f1 + (1-beta)*f2)/2\n",
    "    off2 = ((1-beta)*f1 + (1+beta)*f2)/2\n",
    "    return nn.Parameter(off1), nn.Parameter(off2)\n",
    "    \n",
    "    \n",
    "def mate(pNetwork1, pNetwork2, beta = 1.1):\n",
    "    '''\n",
    "        Crossover operator: simulated binary crossover\n",
    "    '''\n",
    "    encoderWeights1, decoderWeights1 = getWeights(pNetwork1)\n",
    "    encoderWeights2, decoderWeights2 = getWeights(pNetwork2)\n",
    "    \n",
    "#     offspringNetwork1 = Autoencoder()\n",
    "#     offspringNetwork2 = Autoencoder()\n",
    "    \n",
    "    pNetwork1[0].fc1.weight,  pNetwork2[0].fc1.weight = simulatedBinaryCrossover(encoderWeights1['weights'], encoderWeights2['weights'], beta)\n",
    "    pNetwork1[0].fc1.bias, pNetwork2[0].fc1.bias = simulatedBinaryCrossover(encoderWeights1['bias'], encoderWeights2['bias'], beta)\n",
    "    \n",
    "    pNetwork1[0].fc2.weight, pNetwork2[0].fc2.weight = simulatedBinaryCrossover(decoderWeights1['weights'], decoderWeights2['weights'], beta)\n",
    "    pNetwork1[0].fc2.bias, pNetwork2[0].fc2.bias = simulatedBinaryCrossover(decoderWeights1['bias'], decoderWeights2['bias'], beta)\n",
    "    \n",
    "    return pNetwork1, pNetwork2\n",
    "\n",
    "def sumNormal(tensor, alpha = 0.3):\n",
    "    return nn.Parameter(tensor + alpha*torch.randn(tensor.shape))\n",
    "\n",
    "def mutate(network):\n",
    "    encoderWeights, decoderWeights = getWeights(network)\n",
    "    \n",
    "    network[0].fc1.weight = sumNormal(encoderWeights['weights'])\n",
    "    network[0].fc1.bias = sumNormal(encoderWeights['bias'])\n",
    "    \n",
    "    network[0].fc2.weight = sumNormal(decoderWeights['weights'])\n",
    "    network[0].fc2.bias = sumNormal(decoderWeights['bias'])\n",
    "    \n",
    "    return network,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import random\n",
    "\n",
    "# CONSTANTS\n",
    "\n",
    "# GENETIC ALGORITHM CONSTANTS:\n",
    "POPULATION_SIZE = 50\n",
    "P_CROSSOVER = 0.9\n",
    "P_MUTATION = 0.2\n",
    "MAX_GENERATIONS = 50 \n",
    "RANDOM_SEED = 42; random.seed(RANDOM_SEED)\n",
    "\n",
    "# DEFINING THE PROBLEM\n",
    "toolbox = base.Toolbox()\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights = (-1.0,))\n",
    "creator.create(\"individual\", list, fitness = creator.FitnessMin)\n",
    "toolbox.register(\"IndividualCreator\", tools.initIterate, creator.individual, getIndividual)\n",
    "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.IndividualCreator)\n",
    "\n",
    "toolbox.register(\"evaluate\", getLoss)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize = 10)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmin    \tavg    \n",
      "0  \t50    \t4.60223\t4.64516\n",
      "1  \t43    \t4.60223\t5.21025\n",
      "2  \t42    \t4.60216\t5.01565\n",
      "3  \t48    \t4.60208\t5.32073\n",
      "4  \t45    \t4.60208\t5.217  \n",
      "5  \t50    \t4.60208\t5.13698\n",
      "6  \t46    \t4.60208\t5.01139\n",
      "7  \t48    \t4.60208\t5.08927\n",
      "8  \t40    \t4.60208\t5.10626\n",
      "9  \t47    \t4.60208\t5.11705\n",
      "10 \t50    \t4.60208\t5.0486 \n",
      "11 \t46    \t4.60208\t5.4673 \n",
      "12 \t42    \t4.60208\t5.01858\n",
      "13 \t44    \t4.60208\t5.3528 \n",
      "14 \t48    \t4.60208\t5.55297\n",
      "15 \t45    \t4.60208\t5.3212 \n",
      "16 \t46    \t4.60208\t4.99654\n",
      "17 \t47    \t4.60208\t5.10279\n",
      "18 \t47    \t4.60208\t5.4658 \n",
      "19 \t46    \t4.60208\t5.2592 \n",
      "20 \t44    \t4.60208\t4.83509\n",
      "21 \t46    \t4.60208\t5.25324\n",
      "22 \t49    \t4.60208\t5.34735\n",
      "23 \t44    \t4.60208\t5.23081\n",
      "24 \t46    \t4.60208\t5.34311\n",
      "25 \t45    \t4.60208\t5.31846\n",
      "26 \t49    \t4.60208\t5.27838\n",
      "27 \t46    \t4.60208\t5.00487\n",
      "28 \t48    \t4.60208\t5.30619\n",
      "29 \t49    \t4.60208\t5.07277\n",
      "30 \t45    \t4.60208\t5.41122\n",
      "31 \t43    \t4.60208\t5.17193\n",
      "32 \t42    \t4.60208\t5.36054\n",
      "33 \t48    \t4.60208\t5.5236 \n",
      "34 \t49    \t4.60208\t5.12059\n",
      "35 \t44    \t4.60208\t5.18428\n",
      "36 \t46    \t4.60208\t5.21245\n",
      "37 \t40    \t4.60208\t5.09519\n",
      "38 \t43    \t4.60208\t5.01161\n",
      "39 \t46    \t4.60208\t5.21846\n",
      "40 \t49    \t4.60208\t5.81361\n",
      "41 \t42    \t4.60208\t4.90438\n",
      "42 \t50    \t4.60208\t5.07318\n",
      "43 \t40    \t4.60208\t5.1873 \n",
      "44 \t42    \t4.60208\t5.18053\n",
      "45 \t47    \t4.60208\t5.23502\n",
      "46 \t47    \t4.60208\t5.2422 \n",
      "47 \t48    \t4.60208\t5.11218\n",
      "48 \t47    \t4.60208\t5.22671\n",
      "49 \t46    \t4.60208\t4.93831\n",
      "50 \t43    \t4.60208\t5.13618\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-0fc4be251ff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# plot statistics:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"whitegrid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxFitnessValues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeanFitnessValues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'green'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from deap import algorithms\n",
    "import seaborn as sns\n",
    "\n",
    "population = toolbox.populationCreator(n = POPULATION_SIZE)\n",
    "\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"avg\", np.mean)\n",
    "\n",
    "population, logbook = algorithms.eaSimple(population, toolbox, cxpb = P_CROSSOVER, mutpb = P_MUTATION,\n",
    "                                          ngen = MAX_GENERATIONS, stats = stats, verbose = True)\n",
    "\n",
    "# Genetic Algorithm is done - extract statistics:\n",
    "maxFitnessValues, meanFitnessValues = logbook.select(\"max\", \"avg\")\n",
    "\n",
    "# plot statistics:\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(maxFitnessValues, color = 'red')\n",
    "plt.plot(meanFitnessValues, color = 'green')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Max / Average Fitness')\n",
    "plt.title('Max and Average Fitness over Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
>>>>>>> 269b942c4552a36db527f001227eaeab4fed2a8d
}

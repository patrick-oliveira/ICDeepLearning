\documentclass[twocolumn]{article}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{color}
\usepackage{authblk}
\usepackage[colorlinks,citecolor=red,urlcolor=blue,bookmarks=false,hypertexnames=true]{hyperref}
\usepackage{geometry}
\usepackage{float}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}

\newcommand{\limite}{\displaystyle\lim}
\newcommand{\integral}{\displaystyle\int}
\newcommand{\somatorio}{\displaystyle\sum}
\newtheorem{teorema}{Teorema}[section]
\newtheorem{definicao}{Definição}[section]
\newtheorem{proposicao}{Proposição}[section]
\newtheorem{corolario}[teorema]{Corolário}
\newtheorem{lema}{Lema}[section]
\newtheorem{exemplo}{Exemplo}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{collorary}[theorem]{Collorary}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Propositon}[section]
\newtheorem{example}{Example}[section]

% Dados de identificação
\title{Linear Regression Model}
\author{Patrick Oliveira}
\affil{}

\begin{document}
\maketitle

\section{Introduction}
Notes taken from the Chapter 2 of Haykin's book.

\subsection{Defining some basic concepts}

A regression problem consisting in finding a relation between random variables,

\begin{itemize}
	\item One of the random variables is considered to be of particular interest; that rnadom variable is referred to as a dpeendent variable, or response.
	\item The remaining random variables are called independent variables, or regressors; their role is to explain or predict the statistical behavior of the response.
	\item The dependence of the response on the regressors includes an additive error term, to account for uncertainties in the manner in which this dependence is formulated; the error term is called the expectational error, or explainational error, both of which are used interchangeably.
\end{itemize}

The regressor is a set o inputs given by

$$ \vec{x} = \left[ x_{1}, x_{2}, \ldots, x_{M} \right]^{T} $$

The response, supposing a linear relation (linear regression), is given by

$$ d = \sum_{j = 1}^{M}w_{j}x_{j} + \varepsilon $$

\noindent where $ w_{1}, w_{2},\ldots,w_{M} $ is a set of fixed, unknown parameters, with the implicit assumption that the output is stationary. In vector notation,

$$ d = \vec{w}^{T} \vec{x} + \varepsilon $$

\noindent where

$$ \vec{w} = \left[ w_{1}, w_{2}, \ldots, w_{M} \right]^{T} $$

\noindent is the parameter vector. The regressor $ \vec{x} $, the response $ d $ and the expectational error $ \varepsilon $ are sample values of the random variables $ \vec{X} $, $ D $ and $ E $, respectively. With those concepts we define the problem

\begin{definition}
	Given the joint statistics of the regressor $ \vec{X} $ and the corresponding response $ D $, estimate the unknown parameter vector $ \vec{w} $.
\end{definition}

By joint statistics, we understand the correlation matrix $ \Sigma $ of the regressor $ \vec{X} $, the variance of the desired response $D$ and the cross correlation vector between $ \vec{X} $ and the desired responde $ D $. It's assumed that the means of $ \vec{X} $ and $ D $ are both zero.


\end{document}

